{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, pearsonr, kendalltau,  entropy, spearmanr, linregress, rankdata, ttest_rel, ttest_1samp, ttest_ind\n",
    "from statsmodels.stats import multitest\n",
    "import scipy.linalg as la\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from statsmodels.stats import multitest\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "# %% 불용어 리스트 불러오기\n",
    "# input은 커스텀 불용어\n",
    "def load_stopword(input_list=None):\n",
    "    stopwords_f = open('stopwords.txt','rt', encoding='UTF8')\n",
    "    stopwords_list = stopwords_f.readlines()\n",
    "    stopwords_f.close()\n",
    "    stopwords = []\n",
    "    for words in stopwords_list:\n",
    "        words = words.replace(\"\\n\", '')\n",
    "        stopwords.append(words.split('/'))\n",
    "    if input_list:\n",
    "        for words in input_list: stopwords.append([words[0], words[1]])\n",
    "    return(stopwords)\n",
    "\n",
    "\n",
    "    \n",
    "# %% 구어 태깅\n",
    "#  구어 태깅\n",
    "def etri_spokentagger(input, stopwords=False):\n",
    "    \"\"\"_ETRI 구어 태깅_\n",
    "\n",
    "    Args:\n",
    "        input (_str_): 텍스트\n",
    "        stopwords (optional): 불용어 제외 여부\n",
    "            Defaults to True, 기본 불용어 (load_stopword)\n",
    "            False: 하지 않음\n",
    "            list: [[\"단어\", \"품사]] 형태의 커스텀 불용어\n",
    "    \"\"\"\n",
    "    import urllib3\n",
    "    import json\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU_spoken\"\n",
    "    accessKey = \"9d3f4f60-ac44-43c9-9ae4-a3df0ee86686\"  # 개인 accessKey\n",
    "    analysisCode = \"morp\"\n",
    "    requestJson = {\n",
    "    \"argument\": {\n",
    "        \"text\": input,\n",
    "        \"analysis_code\": analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\" :  accessKey},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    results = response.data.decode('utf-8')\n",
    "    results = json.loads(results)['return_object']['sentence']\n",
    "    tagged_results = []\n",
    "    # 불용어 제거\n",
    "    if stopwords:\n",
    "        stopwords_list = load_stopword()\n",
    "        stopword_tag = []\n",
    "        for word in stopwords_list:\n",
    "            if not word[0]: stopword_tag.append(word[1])\n",
    "    \n",
    "    # 결과값 저장\n",
    "    for sent_result in results:\n",
    "        sentence = []\n",
    "        for word in sent_result['morp']:\n",
    "            if stopwords:\n",
    "                if word['type'] in stopword_tag: pass\n",
    "                elif [word['lemma'], word['type']] in stopwords_list: pass\n",
    "                else: sentence.append([word['lemma'], word['type']])\n",
    "            else: sentence.append([word['lemma'], word['type']])\n",
    "        tagged_results.extend(sentence)  # rather use extend  \n",
    "    return(tagged_results)\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm     \n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def extract_recalledidx(group):\n",
    "    keys = [f'sub-0{group}01',f'sub-0{group}02',f'sub-0{group}03']\n",
    "\n",
    "    data_transcript = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV_clean/group-0{str(group)}/group-0{str(group)} sharedeb.xlsx')\n",
    "    sharedeb = np.array(data_transcript['sharedeb'])\n",
    "    data_recs = np.array(data_transcript[keys])\n",
    "\n",
    "    recalled_evidx=[]\n",
    "    recalled_evs = []\n",
    "    for i in range(len(sharedeb)):\n",
    "        count=0\n",
    "        for j in range(3):\n",
    "            if (type(data_recs[i,j])==str or np.isnan(data_recs[i,j])==False) and (data_recs[i,j]!=0):\n",
    "                count+=1\n",
    "        if count==3:\n",
    "            if sharedeb[i] not in recalled_evidx:\n",
    "                recalled_evidx.append(sharedeb[i])\n",
    "                recalled_evs.append(data_recs[i,:])\n",
    "    return recalled_evs\n",
    "\n",
    "def extract_recalledidx_all(group):\n",
    "    keys = [f'sub-0{group}01',f'sub-0{group}02',f'sub-0{group}03']\n",
    "\n",
    "    data_transcript = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV_clean/group-0{str(group)}/group-0{str(group)} sharedeb.xlsx')\n",
    "    sharedeb = np.array(data_transcript['sharedeb'])\n",
    "    data_recs = np.array(data_transcript[keys])\n",
    "    recalled_evidx_all_oh = np.zeros((sharedeb[-1],3))\n",
    "    recalled_evs_all = np.zeros((sharedeb[-1],3), dtype='object')\n",
    "    for i in range(len(sharedeb)):\n",
    "        for j in range(3):\n",
    "            if (type(data_recs[i,j])==str or np.isnan(data_recs[i,j])==False) and (data_recs[i,j]!=0):\n",
    "                recalled_evidx_all_oh[int(sharedeb[i])-1,j] = 1\n",
    "                recalled_evs_all[int(sharedeb[i])-1,j] = str(data_recs[i,j])\n",
    "    return recalled_evidx_all_oh, recalled_evs_all\n",
    "\n",
    "def extract_recalledidx_all_sharedeb(group):\n",
    "    keys = [f'sub-0{group}01',f'sub-0{group}02',f'sub-0{group}03']\n",
    "\n",
    "    data_transcript = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV_clean/group-0{str(group)}/group-0{str(group)} sharedeb.xlsx')\n",
    "    sharedeb = np.array(data_transcript['sharedeb'])\n",
    "    data_recs = np.array(data_transcript[keys])\n",
    "    recalled_evidx_all_oh = np.zeros((sharedeb[-1],3))\n",
    "    recalled_evs_all = np.zeros((sharedeb[-1],3), dtype='object')\n",
    "    for i in range(len(sharedeb)):\n",
    "        for j in range(3):\n",
    "            if (type(data_recs[i,j])==str or np.isnan(data_recs[i,j])==False) and (data_recs[i,j]!=0):\n",
    "                recalled_evidx_all_oh[int(sharedeb[i])-1,j] = 1\n",
    "                recalled_evs_all[int(sharedeb[i])-1,j] = str(data_recs[i,j])\n",
    "    recalled_evs_all = np.column_stack((np.arange(1,max(sharedeb)+1),recalled_evs_all))\n",
    "    return recalled_evidx_all_oh, recalled_evs_all\n",
    "\n",
    "def get_boundary(group,sub):\n",
    "    groupsub = group+sub\n",
    "    word_data = pd.read_excel(f'/home/jiunchoi/OFD/OFD_AUDIO/derivatives/group-{group}/sub-{groupsub}_run-1_day2_words.xlsx')\n",
    "    posthoc_data = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV/group-{group}/sub-{groupsub}_day2_posthoc.xlsx')\n",
    "    # word_data.keys() = start, end, word\n",
    "    # posthoc_data.keys() = sentence, segmentation, description, tag\n",
    "    words = word_data['word']\n",
    "    start = word_data['start']\n",
    "    end = word_data['end']\n",
    "    sentstartend = []\n",
    "    eventstartend = []\n",
    "\n",
    "    segcol = posthoc_data['segmentation']\n",
    "\n",
    "    # 아래가 concat의 원리\n",
    "    tmpstart = start[0]\n",
    "    for i in range (len(words)-1):\n",
    "        if '.' in str(words[i]) or ',' in str(words[i]) or '었고' in str(words[i]) or '것 같고' in str(words[i]) or '지고' in str(words[i]) or '는데' in str(words[i]) or '근데' in str(words[i]) or '그리고' in str(words[i]) or '그러고' in str(words[i]) or '됐고' in str(words[i]) or '했고' in str(words[i]): \n",
    "            tmpend = end[i]\n",
    "            sentstartend.append([tmpstart,tmpend])\n",
    "            tmpstart = start[i+1]\n",
    "    tmpend = end[len(words)-1]\n",
    "    sentstartend.append([tmpstart,tmpend])\n",
    "\n",
    "    tmpstart = sentstartend[0][0]\n",
    "    boundary = []\n",
    "    for i in range (len(segcol)-1):\n",
    "        if segcol[i] != segcol[i+1]:\n",
    "            boundary.append(sentstartend[i][1])            \n",
    "            eventstartend.append([tmpstart,sentstartend[i][1]])\n",
    "            tmpstart = sentstartend[i+1][0]\n",
    "    tmpend = sentstartend[-1][1]\n",
    "    eventstartend.append([tmpstart,tmpend])    \n",
    "    \n",
    "    return eventstartend, boundary\n",
    "\n",
    "def get_boundary2(group,sub,run):\n",
    "    groupsub = '0'+str(group)+'0'+str(sub)\n",
    "    word_data = pd.read_excel(f'/home/jiunchoi/OFD/OFD_AUDIO/derivatives/group-0{group}/sub-{groupsub}_run-{run}_day2_words.xlsx')\n",
    "    posthoc_data = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV_clean/group-0{group}/sub-{groupsub}_day2_posthoc.xlsx')\n",
    "    # word_data.keys() = start, end, word\n",
    "    # posthoc_data.keys() = sentence, segmentation, description, tag\n",
    "    words = word_data['word']\n",
    "    start = word_data['start']\n",
    "    end = word_data['end']\n",
    "    sentstartend = []\n",
    "    eventstartend = []\n",
    "    endidx = [0]\n",
    "    for t in range (len(posthoc_data['Unnamed: 0'])-1):\n",
    "        if posthoc_data['Unnamed: 0'][t]+1 != posthoc_data['Unnamed: 0'][t+1]:\n",
    "            endidx.append(t)\n",
    "    endidx.append(len(posthoc_data['Unnamed: 0'])-1)\n",
    "    if len(endidx)>2:\n",
    "        segcol = np.array(posthoc_data['segmentation'][endidx[run-1]+1:endidx[run]+1])\n",
    "    else:\n",
    "        segcol = np.array(posthoc_data['segmentation'])\n",
    "\n",
    "    tmpstart = start[0]\n",
    "    for i in range (len(words)-1):\n",
    "        if '.' in str(words[i]) or ',' in str(words[i]) or '었고' in str(words[i]) or '것 같고' in str(words[i]) or '지고' in str(words[i]) or '는데' in str(words[i]) or '근데' in str(words[i]) or '그리고' in str(words[i]) or '그러고' in str(words[i]) or '됐고' in str(words[i]) or '했고' in str(words[i]): \n",
    "            tmpend = end[i]\n",
    "            sentstartend.append([tmpstart,tmpend])\n",
    "            tmpstart = start[i+1]\n",
    "    tmpend = end[len(words)-1]\n",
    "    sentstartend.append([tmpstart,tmpend])\n",
    "\n",
    "    # working for subs with only 1 run\n",
    "    tmpstart = sentstartend[0][0]\n",
    "    for i in range (len(segcol)-1):\n",
    "        if segcol[i] != segcol[i+1]:     \n",
    "            tmpend = sentstartend[i][1]     \n",
    "            eventstartend.append([tmpstart,tmpend])\n",
    "            tmpstart = sentstartend[i+1][0]\n",
    "    tmpend = sentstartend[len(segcol)-1][1]\n",
    "    eventstartend.append([tmpstart,tmpend]) \n",
    "    \n",
    "    return eventstartend\n",
    "\n",
    "def r2z(r):\n",
    "    return 0.5 * (np.log(1 + r) - np.log(1 - r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = f'/home/jiunchoi/OFD/OFD_BHV_clean'\n",
    "groups = [2,3,4,5]\n",
    "group_clean_txts = np.load('output/group_clean_txts.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [2,3,4,5]\n",
    "tokenbags = np.zeros((len(groups),3),dtype='object') #group,sub,s/us\n",
    "\n",
    "for g,group in enumerate(groups):\n",
    "    recalled_evs = np.array(extract_recalledidx(group))\n",
    "    for sub in [1,2,3]:\n",
    "        data_transcript = pd.read_excel(f'{DATADIR}/group-0{str(group)}/sub-0{str(group)}0{str(sub)}_day2_posthoc.xlsx')\n",
    "        recall_segmentation = np.array(data_transcript['segmentation'])\n",
    "        sub_ev = np.zeros(int(recall_segmentation[-1]),dtype='object')\n",
    "        for e in range(1,int(recall_segmentation[-1])+1):\n",
    "            sentfinderidx = np.where(recall_segmentation==e)[0]\n",
    "            sentlevel = []\n",
    "            for eachsent in group_clean_txts[group-2,sub-1][sentfinderidx]:\n",
    "                sentlevel.extend(eachsent)\n",
    "            sub_ev[e-1] = sentlevel\n",
    "        tokenbags[g,sub-1] = sub_ev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (1484, 69) (1484, 27) (1484, 27) checkpoint2\n",
      "2 (31, 3) (31,) checkpoint3\n",
      "2 checkpoint4\n",
      "2 checkpoint5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/582355612.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 checkpoint6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/582355612.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 checkpoint7\n",
      "2 checkpoint8\n",
      "3 (952, 31) (952, 42) (952, 42) checkpoint2\n",
      "3 (27, 3) (27,) checkpoint3\n",
      "3 checkpoint4\n",
      "3 checkpoint5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/582355612.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 checkpoint6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/582355612.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 checkpoint7\n",
      "3 checkpoint8\n",
      "4 (820, 30) (820, 39) (820, 30) checkpoint2\n",
      "4 (10, 3) (10,) checkpoint3\n",
      "4 checkpoint4\n",
      "4 checkpoint5\n",
      "4 checkpoint6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/582355612.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n",
      "/tmp/ipykernel_523168/582355612.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 checkpoint7\n",
      "4 checkpoint8\n",
      "5 (773, 26) (773, 20) (773, 26) checkpoint2\n",
      "5 (22, 3) (22,) checkpoint3\n",
      "5 checkpoint4\n",
      "5 checkpoint5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/582355612.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 checkpoint6\n",
      "5 checkpoint7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/582355612.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 checkpoint8\n"
     ]
    }
   ],
   "source": [
    "dfBAG = []\n",
    "posh = []\n",
    "for g,group in enumerate([2,3,4,5]):\n",
    "    BOW_of_group = []\n",
    "    for sub in [1,2,3]:\n",
    "        BOW_of_group.extend(itertools.chain.from_iterable(list(tokenbags[g,sub-1])))\n",
    "        A = list(set(tuple(i) for i in BOW_of_group))\n",
    "\n",
    "    evBOW_of_group = []\n",
    "    for sub in [1,2,3]:\n",
    "        subn_evtotal_oh = np.zeros((len(A),len(tokenbags[g,sub-1])))\n",
    "        for ev in range(len(tokenbags[g,sub-1])):\n",
    "            subn_evn = set(tuple(i) for i in tokenbags[g,sub-1][ev])\n",
    "\n",
    "            ev_label = []\n",
    "            for idx,answer in enumerate(A):\n",
    "                for pair in subn_evn:\n",
    "                    if answer==pair:\n",
    "                        ev_label.append(idx)\n",
    "            for tokenidx in ev_label:\n",
    "                subn_evtotal_oh[tokenidx,ev] = 1\n",
    "        evBOW_of_group.append(subn_evtotal_oh)\n",
    "    print(group, evBOW_of_group[0].shape, evBOW_of_group[1].shape, evBOW_of_group[2].shape, 'checkpoint2')\n",
    "\n",
    "    # shared ev면 그것끼리 mean을 해 (이걸 len(recalled_evs) 길이의 3차원으로 만드셈)\n",
    "    recalled_evs = np.array(extract_recalledidx(group))\n",
    "    event_word_representation = np.zeros_like(recalled_evs, dtype='object')\n",
    "    for e,ev in enumerate(recalled_evs):\n",
    "        tmp_ev_word_representation = []\n",
    "        for sub in [1,2,3]:\n",
    "            event_no = str(ev[sub-1]).split(',') #list of str\n",
    "            for en in event_no:\n",
    "                tmp_ev_word_representation.append(evBOW_of_group[sub-1][:,int(float(en)-1)]) #이게 (1465,) vector\n",
    "            subtmp_ev_word_representation = np.array(tmp_ev_word_representation)\n",
    "            sub_ev_representation = np.mean(subtmp_ev_word_representation, axis=0)\n",
    "            event_word_representation[e,sub-1] = sub_ev_representation\n",
    "    mean_sharedev_word_representation = np.mean(event_word_representation,axis=1)\n",
    "    print(group, recalled_evs.shape, mean_sharedev_word_representation.shape, 'checkpoint3')\n",
    "\n",
    "    # 위의 sharedev에서 가장 비슷한 (본인 구하는데 들어간거 제외) - 매트릭스유사도비교 (euclidean distance/cossim)\n",
    "    recalled_evidx_all_oh, recalled_evs_all = extract_recalledidx_all(group)\n",
    "    recalled_ppls_word_representation = np.zeros_like(recalled_evidx_all_oh, dtype='object')\n",
    "\n",
    "    for i,sharedev in enumerate(recalled_evs_all):\n",
    "        tmp_shared_word_representation = []\n",
    "        recalled_ppl = np.where(recalled_evidx_all_oh[i]==1)[0]\n",
    "        # print(f'{i}번째 이벤트를 {recalled_ppl} 참가자가 리콜하였습니다')\n",
    "        if len(recalled_ppl) > 0:\n",
    "            for pplidx in recalled_ppl:\n",
    "                event_no = str(sharedev[pplidx]).split(',') #list of str\n",
    "                for en in event_no:\n",
    "                    tmp_shared_word_representation.append(evBOW_of_group[pplidx][:,int(float(en)-1)])\n",
    "                subtmp_ev_word_representation = np.array(tmp_shared_word_representation)\n",
    "                sub_ev_representation = np.mean(subtmp_ev_word_representation, axis=0)\n",
    "                recalled_ppls_word_representation[i,pplidx] = sub_ev_representation\n",
    "    print(group, 'checkpoint4')\n",
    "\n",
    "    # 이제 한줄짜리 만들거임. 셋 다 리콜한건 nan처리, 셋다리콜하지않은것도 nan처리\n",
    "    mean_recalled_ppls_word_representation = np.zeros(len(recalled_ppls_word_representation),dtype='object')\n",
    "    for ev,rpwr_row in enumerate(recalled_ppls_word_representation):\n",
    "        recalled_ppl = np.where(recalled_evidx_all_oh[ev]==1)[0]\n",
    "        if len(recalled_ppl) == 3:\n",
    "            mean_recalled_ppls_word_representation[ev] = 'nan'\n",
    "        elif len(recalled_ppl) == 0:\n",
    "            mean_recalled_ppls_word_representation[ev] = 'nan'\n",
    "        else:\n",
    "            mean_recalled_ppls_word_representation[ev] = np.mean(rpwr_row[recalled_evidx_all_oh[ev]==1])\n",
    "    print(group, 'checkpoint5')\n",
    "\n",
    "    # 이제 셋다리콜한것과 원핫벡터가 제일 비슷한 1,2명 shared event를 찾자! <<근데 계속 나오는 이벤트만 나오는게 좀 걸림\n",
    "    pair_of_sharedev = np.zeros((len(mean_sharedev_word_representation),2), dtype='object')\n",
    "    for m,mswp in enumerate(mean_sharedev_word_representation):\n",
    "        tmp_cossim = np.zeros(len(mean_recalled_ppls_word_representation))\n",
    "        for mr, mrpwr in enumerate(mean_recalled_ppls_word_representation):\n",
    "            if type(mswp)==np.ndarray and mrpwr!='nan':\n",
    "                tmp_cossim[mr] = cos_similarity(mswp,mrpwr)\n",
    "\n",
    "        candid = np.where(tmp_cossim == np.max(tmp_cossim))[0]\n",
    "        ball = 1\n",
    "        while ball==1:\n",
    "            for r, reac in enumerate(recalled_evs_all[candid]):\n",
    "                for s in range(3):\n",
    "                    for cand in candid:\n",
    "                        testee = str(reac[s]).split(',')\n",
    "                        for tt in testee:\n",
    "                            for res in (recalled_evs):\n",
    "                                ev = str(res[s]).split(',')\n",
    "                                for e in ev:\n",
    "                                    if int(float(tt)) == int(float(e)):\n",
    "                                        tmp_cossim[cand] = 0\n",
    "                                    else:\n",
    "                                        candid = np.where(tmp_cossim == np.max(tmp_cossim))[0]\n",
    "                                        ball = 0\n",
    "        pair_of_sharedev[m,0] = candid\n",
    "        pair_of_sharedev[m,1] = tmp_cossim[candid]\n",
    "    posh.append(pair_of_sharedev[:,0])\n",
    "    print(group, 'checkpoint6')\n",
    "\n",
    "    # 그리고 null distri도 만드셈 부트스트랩을 하든지\n",
    "    idxlist = []\n",
    "    for idx,mrpwr in enumerate(mean_recalled_ppls_word_representation):\n",
    "        if mrpwr=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            idxlist.append(idx)\n",
    "\n",
    "    n_permutation = 1000\n",
    "    permuted_stats = np.zeros((n_permutation)) #save euclidean distance \n",
    "    for i in range(n_permutation):\n",
    "        for m,mswr in enumerate(mean_sharedev_word_representation):\n",
    "            for j in pair_of_sharedev[m,0]:\n",
    "                try:\n",
    "                    idxlist.remove(j)\n",
    "                except:\n",
    "                    pass\n",
    "        nonpair_idx = random.choice(idxlist)\n",
    "        permuted_stats[i] = cos_similarity(mswr,mean_recalled_ppls_word_representation[nonpair_idx])\n",
    "    print(group, 'checkpoint7')\n",
    "\n",
    "    # 위의 짓을 겁나많이해서 tmp_distance의 가능한 분포가 바이올린플롯으로 있고\n",
    "    peridx = np.zeros_like(permuted_stats)\n",
    "    pairidx = np.ones_like(pair_of_sharedev[:,1])\n",
    "    a = np.vstack((peridx,permuted_stats))\n",
    "    try:\n",
    "        b = np.vstack((pairidx, pair_of_sharedev[:,1].astype('float')))\n",
    "    except:\n",
    "            pair_of_sharedev_dist = np.zeros(len(pair_of_sharedev[:,1]))\n",
    "            for p,pose in enumerate(pair_of_sharedev[:,1]):\n",
    "                if len(pose)>1:\n",
    "                    pair_of_sharedev_dist[p] = pose[0]\n",
    "                else:\n",
    "                    pair_of_sharedev_dist[p] = pose\n",
    "            b = np.vstack((pairidx, pair_of_sharedev_dist))\n",
    "\n",
    "    c = np.hstack((a,b)).T\n",
    "    df = pd.DataFrame(c, columns=['what','distance'],dtype='float')\n",
    "    dfBAG.append(df)\n",
    "    print(group, 'checkpoint8')\n",
    "    # time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523168/243270408.py:77: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n",
      "/tmp/ipykernel_523168/243270408.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n",
      "/tmp/ipykernel_523168/243270408.py:77: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n",
      "/tmp/ipykernel_523168/243270408.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n",
      "/tmp/ipykernel_523168/243270408.py:77: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n",
      "/tmp/ipykernel_523168/243270408.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n",
      "/tmp/ipykernel_523168/243270408.py:77: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if type(mswp)==np.ndarray and mrpwr!='nan':\n",
      "/tmp/ipykernel_523168/243270408.py:102: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if mrpwr=='nan':\n"
     ]
    }
   ],
   "source": [
    "allcossim = []\n",
    "dfBAG = []\n",
    "posh = []\n",
    "\n",
    "for g,group in enumerate([2,3,4,5]):\n",
    "    BOW_of_group = []\n",
    "    for sub in [1,2,3]:\n",
    "        BOW_of_group.extend(itertools.chain.from_iterable(list(tokenbags[g,sub-1])))\n",
    "        A = list(set(tuple(i) for i in BOW_of_group))\n",
    "\n",
    "    evBOW_of_group = []\n",
    "    for sub in [1,2,3]:\n",
    "        subn_evtotal_oh = np.zeros((len(A),len(tokenbags[g,sub-1])))\n",
    "        for ev in range(len(tokenbags[g,sub-1])):\n",
    "            subn_evn = set(tuple(i) for i in tokenbags[g,sub-1][ev])\n",
    "\n",
    "            ev_label = []\n",
    "            for idx,answer in enumerate(A):\n",
    "                for pair in subn_evn:\n",
    "                    if answer==pair:\n",
    "                        ev_label.append(idx)\n",
    "            for tokenidx in ev_label:\n",
    "                subn_evtotal_oh[tokenidx,ev] = 1\n",
    "        evBOW_of_group.append(subn_evtotal_oh)\n",
    "    # print(group, evBOW_of_group[0].shape, evBOW_of_group[1].shape, evBOW_of_group[2].shape, 'checkpoint2')\n",
    "\n",
    "    # shared ev면 그것끼리 mean을 해 (이걸 len(recalled_evs) 길이의 3차원으로 만드셈)\n",
    "    recalled_evs = np.array(extract_recalledidx(group))\n",
    "    event_word_representation = np.zeros_like(recalled_evs, dtype='object')\n",
    "    for e,ev in enumerate(recalled_evs):\n",
    "        tmp_ev_word_representation = []\n",
    "        for sub in [1,2,3]:\n",
    "            event_no = str(ev[sub-1]).split(',') #list of str\n",
    "            for en in event_no:\n",
    "                tmp_ev_word_representation.append(evBOW_of_group[sub-1][:,int(float(en)-1)]) #이게 (1465,) vector\n",
    "            subtmp_ev_word_representation = np.array(tmp_ev_word_representation)\n",
    "            sub_ev_representation = np.mean(subtmp_ev_word_representation, axis=0)\n",
    "            event_word_representation[e,sub-1] = sub_ev_representation\n",
    "    mean_sharedev_word_representation = np.mean(event_word_representation,axis=1)\n",
    "    # print(group, recalled_evs.shape, mean_sharedev_word_representation.shape, 'checkpoint3')\n",
    "\n",
    "    # 위의 sharedev에서 가장 비슷한 (본인 구하는데 들어간거 제외) - 매트릭스유사도비교 (euclidean distance/cossim)\n",
    "    recalled_evidx_all_oh, recalled_evs_all = extract_recalledidx_all(group)\n",
    "    recalled_ppls_word_representation = np.zeros_like(recalled_evidx_all_oh, dtype='object')\n",
    "\n",
    "    for i,sharedev in enumerate(recalled_evs_all):\n",
    "        tmp_shared_word_representation = []\n",
    "        recalled_ppl = np.where(recalled_evidx_all_oh[i]==1)[0]\n",
    "        # print(f'{i}번째 이벤트를 {recalled_ppl} 참가자가 리콜하였습니다')\n",
    "        if len(recalled_ppl) > 0:\n",
    "            for pplidx in recalled_ppl:\n",
    "                event_no = str(sharedev[pplidx]).split(',') #list of str\n",
    "                for en in event_no:\n",
    "                    tmp_shared_word_representation.append(evBOW_of_group[pplidx][:,int(float(en)-1)])\n",
    "                subtmp_ev_word_representation = np.array(tmp_shared_word_representation)\n",
    "                sub_ev_representation = np.mean(subtmp_ev_word_representation, axis=0)\n",
    "                recalled_ppls_word_representation[i,pplidx] = sub_ev_representation\n",
    "    # print(group, 'checkpoint4')\n",
    "\n",
    "    # 이제 한줄짜리 만들거임. 셋 다 리콜한건 nan처리, 셋다리콜하지않은것도 nan처리\n",
    "    mean_recalled_ppls_word_representation = np.zeros(len(recalled_ppls_word_representation),dtype='object')\n",
    "    for ev,rpwr_row in enumerate(recalled_ppls_word_representation):\n",
    "        recalled_ppl = np.where(recalled_evidx_all_oh[ev]==1)[0]\n",
    "        if len(recalled_ppl) == 3:\n",
    "            mean_recalled_ppls_word_representation[ev] = 'nan'\n",
    "        elif len(recalled_ppl) == 0:\n",
    "            mean_recalled_ppls_word_representation[ev] = 'nan'\n",
    "        else:\n",
    "            mean_recalled_ppls_word_representation[ev] = np.mean(rpwr_row[recalled_evidx_all_oh[ev]==1])\n",
    "    # print(group, 'checkpoint5')\n",
    "\n",
    "    # 이제 셋다리콜한것과 원핫벡터가 제일 비슷한 1,2명 shared event를 찾자! <<근데 계속 나오는 이벤트만 나오는게 좀 걸림\n",
    "    pair_of_sharedev = np.zeros((len(mean_sharedev_word_representation),2), dtype='object')\n",
    "    for m,mswp in enumerate(mean_sharedev_word_representation):\n",
    "        tmp_cossim = np.zeros(len(mean_recalled_ppls_word_representation))\n",
    "        for mr, mrpwr in enumerate(mean_recalled_ppls_word_representation):\n",
    "            if type(mswp)==np.ndarray and mrpwr!='nan':\n",
    "                tmp_cossim[mr] = cos_similarity(mswp,mrpwr)\n",
    "                allcossim.append(tmp_cossim)\n",
    "\n",
    "        candid = np.where(tmp_cossim == np.max(tmp_cossim))[0]\n",
    "        ball = len(candid)\n",
    "        while ball==len(candid):\n",
    "            for cand in candid:\n",
    "                if (np.sum(recalled_evidx_all_oh[cand])!=3) and (np.sum(recalled_evidx_all_oh[cand])!=0): #셋 다 말한 게 아니고 셋 다 안말한 것도 아니고\n",
    "                    row = recalled_evs_all[cand]\n",
    "                    for s,eachsub in enumerate(row):\n",
    "                        esrec = str(eachsub).split(',') #recalled_evs_all에서 cand에 해당하는 줄\n",
    "                        ms = str(mswp[s]).split(',') #얘랑 cossim을 계산했던 shared 한 줄\n",
    "                        for esr in esrec: #참가자리콜 중 겹치는 event pattern이 없는지\n",
    "                            for mss in ms:\n",
    "                                if int(float(esr))!=int(float(mss)):\n",
    "                                    ball-=1 # 저장을 해\n",
    "\n",
    "        pair_of_sharedev[m,0] = candid\n",
    "        pair_of_sharedev[m,1] = tmp_cossim[candid]\n",
    "    posh.append(pair_of_sharedev[:,0])\n",
    "\n",
    "    # 그리고 null distri도 만드셈 부트스트랩을 하든지\n",
    "    idxlist = []\n",
    "    for idx,mrpwr in enumerate(mean_recalled_ppls_word_representation):\n",
    "        if mrpwr=='nan':\n",
    "            pass\n",
    "        else:\n",
    "            idxlist.append(idx)\n",
    "\n",
    "    n_permutation = 1000\n",
    "    permuted_stats = np.zeros((n_permutation)) #save euclidean distance \n",
    "    for i in range(n_permutation):\n",
    "        for m,mswr in enumerate(mean_sharedev_word_representation):\n",
    "            for j in pair_of_sharedev[m,0]:\n",
    "                try:\n",
    "                    idxlist.remove(j)\n",
    "                except:\n",
    "                    pass\n",
    "        nonpair_idx = random.choice(idxlist)\n",
    "        permuted_stats[i] = cos_similarity(mswr,mean_recalled_ppls_word_representation[nonpair_idx])\n",
    "    # print(group, 'checkpoint7')\n",
    "\n",
    "    # 위의 짓을 겁나많이해서 tmp_distance의 가능한 분포가 바이올린플롯으로 있고\n",
    "    peridx = np.zeros_like(permuted_stats)\n",
    "    pairidx = np.ones_like(pair_of_sharedev[:,1])\n",
    "    a = np.vstack((peridx,permuted_stats))\n",
    "    try:\n",
    "        b = np.vstack((pairidx, pair_of_sharedev[:,1].astype('float')))\n",
    "    except:\n",
    "            pair_of_sharedev_dist = np.zeros(len(pair_of_sharedev[:,1]))\n",
    "            for p,pose in enumerate(pair_of_sharedev[:,1]):\n",
    "                if len(pose)>1:\n",
    "                    pair_of_sharedev_dist[p] = pose[0]\n",
    "                else:\n",
    "                    pair_of_sharedev_dist[p] = pose\n",
    "            b = np.vstack((pairidx, pair_of_sharedev_dist))\n",
    "\n",
    "    c = np.hstack((a,b)).T\n",
    "    df = pd.DataFrame(c, columns=['what','distance'],dtype='float')\n",
    "    dfBAG.append(df)\n",
    "    # print(group, 'checkpoint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = sns.color_palette(\"Set2\")\n",
    "# plt.figure(figsize=(3,4),dpi=300)\n",
    "# sns.violinplot(data= pd.concat(dfBAG), x='what',y='distance', pallette=colors)\n",
    "# # plt.ylim(1,17)\n",
    "\n",
    "# # 그럼에도 불구하고 내가 만들어준 페어는 토큰원핫벡터의 유사도가 sig하게 높다는 걸 보임\n",
    "# df = pd.concat(dfBAG)\n",
    "# t_stat, p_values = ttest_ind(df.loc[df['what']==0]['distance'], df.loc[df['what']==1]['distance'])\n",
    "# print(i+2, t_stat, p_values, p_values<0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKDIR = '/home/jiunchoi/OFD/source/'\n",
    "DATADIR = '/home/jiunchoi/OFD/OFD_DATA/derivatives/'\n",
    "BN_atlas = nib.load(f'{MASKDIR}BNA_3mm_atlas.nii').get_fdata()\n",
    "\n",
    "def load_brain(group,sub,run):\n",
    "    task = 'RECALL'\n",
    "    BN_parcels = np.zeros((246), dtype='object')\n",
    "    groupsub = '0'+str(group)+'0'+str(sub)\n",
    "    fmri_data = nib.load(f'{DATADIR}sub-{groupsub}/preprocessed/sub-{groupsub}_{task}_{run}_sc_dt_sm.nii.gz').get_fdata()\n",
    "    for roi in range (1,246+1):\n",
    "        roi_data = fmri_data[BN_atlas==roi, :].T #(Time, Voxels)\n",
    "        roi_data = zscore(roi_data, axis=0)\n",
    "        roi_data = np.nan_to_num(roi_data)\n",
    "        BN_parcels[roi-1] = roi_data\n",
    "    return BN_parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=[2,3,4,5]\n",
    "group_brain = np.zeros((len(groups),3), dtype='object')\n",
    "\n",
    "for group in groups:\n",
    "    for sub in [1,2,3]:\n",
    "        sub_brain = []\n",
    "        if (group==2 and sub==1):\n",
    "            runs = [1,2,3]\n",
    "        else:\n",
    "            runs = [1]\n",
    "        for run in runs:\n",
    "            sub_brain.append(load_brain(group,sub,run))\n",
    "        group_brain[group-2,sub-1] = sub_brain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 그룹 다 돌아가는 최종판!!\n",
    "result = np.zeros((4,246,2),dtype='object') #shared_r, paired_r 몰아넣기\n",
    "result_stat = np.zeros((4,246,2)) # t,p 저장\n",
    "\n",
    "for group in [2,3,4,5]:\n",
    "    recalled_evidx_all_oh, recalled_evs_all = extract_recalledidx_all(int(group))\n",
    "    recalled_evs = extract_recalledidx(int(group))\n",
    "    wherelist = []\n",
    "    for i,reao in enumerate(recalled_evidx_all_oh):\n",
    "        if np.sum(reao)==0 or np.sum(reao)==3:\n",
    "            a = 'none'\n",
    "        else:\n",
    "            a = np.where(reao==1)[0]\n",
    "        wherelist.append(a)\n",
    "\n",
    "    for roi in range(246):\n",
    "        shared_event_patterns = np.zeros((3, len(recalled_evs), group_brain[int(group)-2,int(sub)-1][run-1][roi].shape[1])) #\n",
    "        paired_event_patterns = np.zeros((3, len(posh[int(group)-2]), group_brain[int(group)-2,int(sub)-1][run-1][roi].shape[1])) #\n",
    "         \n",
    "        for sub in [1,2,3]:\n",
    "            if (group==2 and sub==1):\n",
    "                runs = [1,2,3]\n",
    "            else:\n",
    "                runs = [1]\n",
    "            eventstartend = []\n",
    "            runidx = []\n",
    "            for run in runs:\n",
    "                evse = get_boundary2(group,sub,run)\n",
    "                eventstartend.extend(evse)\n",
    "                runidx.extend([run]*len(evse))\n",
    "\n",
    "            # Condition 1: 같은사건 패턴로드\n",
    "            for e in range(len(recalled_evs)):\n",
    "                ev_from_recall = str(recalled_evs[e][sub-1]).split(',')\n",
    "\n",
    "                if len(ev_from_recall) > 1:\n",
    "                        single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                        for efr in ev_from_recall[1:]:\n",
    "                            single_pattern_timepoint = np.vstack((single_pattern_timepoint,eventstartend[int(float(efr))-1]))\n",
    "                        single_pattern = group_brain[group-2,sub-1][runidx[int(float(ev_from_recall[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0,0]/1000):math.ceil(single_pattern_timepoint[0,1]/1000),:]\n",
    "                        for s in range(1, single_pattern_timepoint.shape[-1]):\n",
    "                            single_pattern = np.vstack((single_pattern,group_brain[group-2,sub-1][runidx[math.trunc(float(ev_from_recall[0]))-1]-1][roi][math.ceil(single_pattern_timepoint[s,0]/1000):int(single_pattern_timepoint[s,1]/1000),:]))\n",
    "                else:\n",
    "                    single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                    single_pattern = group_brain[group-2,sub-1][runidx[int(float(ev_from_recall[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0]/1000):math.ceil(single_pattern_timepoint[1]/1000),:]\n",
    "                event_pattern = np.mean(single_pattern,axis=0)\n",
    "                shared_event_patterns[sub-1,e,:] = event_pattern\n",
    "\n",
    "            # Condition 2: 다른사건 패턴로드\n",
    "            for e in range(len(posh[group-2])):\n",
    "                for protagidx in (wherelist[posh[group-2][e][0]]):\n",
    "                    if int(float(protagidx)) == (sub-1):\n",
    "                        ev_from_recall = str(recalled_evs_all[posh[group-2][e][0]][sub-1]).split(',')\n",
    "                        if len(ev_from_recall) > 1:\n",
    "                                single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                                for efr in ev_from_recall[1:]:\n",
    "                                    single_pattern_timepoint = np.vstack((single_pattern_timepoint,eventstartend[int(float(efr))-1]))\n",
    "                                single_pattern = group_brain[group-2,sub-1][runidx[int(float(ev_from_recall[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0,0]/1000):math.ceil(single_pattern_timepoint[0,1]/1000),:]\n",
    "                                for s in range(1, single_pattern_timepoint.shape[-1]):\n",
    "                                    single_pattern = np.vstack((single_pattern,group_brain[group-2,sub-1][runidx[math.trunc(float(ev_from_recall[0]))-1]-1][roi][math.ceil(single_pattern_timepoint[s,0]/1000):int(single_pattern_timepoint[s,1]/1000),:]))\n",
    "                        else:\n",
    "                            single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                            single_pattern = group_brain[group-2,sub-1][runidx[int(float(ev_from_recall[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0]/1000):math.ceil(single_pattern_timepoint[1]/1000),:]\n",
    "                        event_pattern = np.mean(single_pattern,axis=0)\n",
    "                        paired_event_patterns[sub-1,e,:] = event_pattern\n",
    "\n",
    "        # 공통이벤트 계산\n",
    "        shared_r = np.zeros(len(recalled_evs))\n",
    "        for e, ev in enumerate(recalled_evs):\n",
    "            pattern_r = []\n",
    "            protagonistidx = wherelist[posh[int(group)-2][e][0]]\n",
    "            \n",
    "            if len(protagonistidx)==1:\n",
    "                pattern_1 = np.squeeze(shared_event_patterns[protagonistidx, e, :])\n",
    "                for otheridx in [0,1,2]:\n",
    "                    if otheridx!=protagonistidx:\n",
    "                        pattern_2 = np.squeeze(shared_event_patterns[otheridx, e, :])\n",
    "                        pattern_r.append(pearsonr(pattern_1, pattern_2)[0])\n",
    "                pattern_r = np.mean(pattern_r)\n",
    "                shared_r[e] = pattern_r\n",
    "\n",
    "            elif len(protagonistidx)==2:\n",
    "                otheridx = np.delete(np.array([0,1,2]),wherelist[posh[int(group)-2][e][0]])\n",
    "                pattern_2 = np.squeeze(shared_event_patterns[otheridx, e, :])\n",
    "                for protagonistidx_a in protagonistidx:\n",
    "                    pattern_1 = np.squeeze(shared_event_patterns[protagonistidx_a, e, :])\n",
    "                    pattern_r.append(pearsonr(pattern_1, pattern_2)[0])\n",
    "                pattern_r = np.mean(pattern_r)\n",
    "                shared_r[e] = pattern_r\n",
    "\n",
    "        #페어이벤트 계산\n",
    "        pair_r = np.zeros(len(recalled_evs))\n",
    "        for e, ev in enumerate(recalled_evs):\n",
    "            pattern_r = []\n",
    "            protagonistidx = wherelist[posh[int(group)-2][e][0]]\n",
    "            if len(protagonistidx)==1:\n",
    "                pattern_1 = np.squeeze(paired_event_patterns[protagonistidx, e, :])\n",
    "                for otheridx in [0,1,2]:\n",
    "                    if otheridx!=protagonistidx:\n",
    "                        pattern_2 = np.squeeze(shared_event_patterns[otheridx, e, :])\n",
    "                        pattern_r.append(pearsonr(pattern_1, pattern_2)[0])\n",
    "                pattern_r = np.mean(pattern_r)\n",
    "                pair_r[e] = pattern_r\n",
    "\n",
    "            elif len(protagonistidx)==2:\n",
    "                otheridx = np.delete(np.array([0,1,2]),wherelist[posh[int(group)-2][e][0]])\n",
    "                pattern_2 = np.squeeze(shared_event_patterns[otheridx, e, :])\n",
    "                for protagonistidx_a in protagonistidx:\n",
    "                    pattern_1 = np.squeeze(paired_event_patterns[protagonistidx_a, e, :])\n",
    "                    pattern_r.append(pearsonr(pattern_1, pattern_2)[0])\n",
    "                pattern_r = np.mean(pattern_r)\n",
    "                pair_r[e] = pattern_r\n",
    "\n",
    "        result[int(group)-2,roi,0] = shared_r        \n",
    "        result[int(group)-2,roi,1] = pair_r       \n",
    "        result_stat[int(group)-2,roi,:] = ttest_ind(r2z(shared_r), r2z(pair_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 167])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(result_paired[:,1]<0.05)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_paired = np.zeros((246,2))\n",
    "for roi in range(246):\n",
    "    sharedr = np.concatenate([result[0,roi,0],result[1,roi,0],result[2,roi,0],result[3,roi,0]])\n",
    "    pairedr = np.concatenate([result[0,roi,1],result[1,roi,1],result[2,roi,1],result[3,roi,1]])\n",
    "    result_paired[roi,:] = ttest_ind(r2z(sharedr),r2z(pairedr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = np.array(nib.load(f'{MASKDIR}BNA_3mm_atlas.nii').get_fdata())\n",
    "\n",
    "empty_brain = np.zeros((atlas.shape))\n",
    "for i in range(1,247):\n",
    "    # if np.mean(result_stat[:,i-1,1],axis=0) < 0.05:\n",
    "    empty_brain[atlas==i] = result_paired[i-1,0]\n",
    "empty_brain[empty_brain==0.0] = np.nan\n",
    "np.save(\"output/230611 ispc_tmap.npy\", empty_brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = np.array(nib.load(f'{MASKDIR}BNA_3mm_atlas.nii').get_fdata())\n",
    "\n",
    "empty_brain = np.zeros((atlas.shape))\n",
    "for i in range(1,247):\n",
    "    if result_paired[i-1,1] < 0.05:\n",
    "        empty_brain[atlas==i] = result_paired[i-1,0]\n",
    "empty_brain[empty_brain==0.0] = np.nan\n",
    "np.save(\"output/230611 ispc_tmap_thres.npy\", empty_brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.662490447157642"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(result_paired[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_p_paired = np.zeros_like(result_paired[:,1])\n",
    "corrected_p_paired = multitest.multipletests(result_paired[:,1], method='fdr_bh')[1]\n",
    "np.where(corrected_p_paired<0.05)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS-RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 done, checkpoint6\n",
      "2 1 done, checkpoint6\n",
      "2 2 done, checkpoint6\n",
      "2 3 done, checkpoint6\n",
      "2 4 done, checkpoint6\n",
      "2 5 done, checkpoint6\n",
      "2 6 done, checkpoint6\n",
      "2 7 done, checkpoint6\n",
      "2 8 done, checkpoint6\n",
      "2 9 done, checkpoint6\n",
      "2 10 done, checkpoint6\n",
      "2 11 done, checkpoint6\n",
      "2 12 done, checkpoint6\n",
      "2 13 done, checkpoint6\n",
      "2 14 done, checkpoint6\n",
      "2 15 done, checkpoint6\n",
      "2 16 done, checkpoint6\n",
      "2 17 done, checkpoint6\n",
      "2 18 done, checkpoint6\n",
      "2 19 done, checkpoint6\n",
      "2 20 done, checkpoint6\n",
      "2 21 done, checkpoint6\n",
      "2 22 done, checkpoint6\n",
      "2 23 done, checkpoint6\n",
      "2 24 done, checkpoint6\n",
      "2 25 done, checkpoint6\n",
      "2 26 done, checkpoint6\n",
      "2 27 done, checkpoint6\n",
      "2 28 done, checkpoint6\n",
      "2 29 done, checkpoint6\n",
      "2 30 done, checkpoint6\n",
      "2 31 done, checkpoint6\n",
      "2 32 done, checkpoint6\n",
      "2 33 done, checkpoint6\n",
      "2 34 done, checkpoint6\n",
      "2 35 done, checkpoint6\n",
      "2 36 done, checkpoint6\n",
      "2 37 done, checkpoint6\n",
      "2 38 done, checkpoint6\n",
      "2 39 done, checkpoint6\n",
      "2 40 done, checkpoint6\n",
      "2 41 done, checkpoint6\n",
      "2 42 done, checkpoint6\n",
      "2 43 done, checkpoint6\n",
      "2 44 done, checkpoint6\n",
      "2 45 done, checkpoint6\n",
      "2 46 done, checkpoint6\n",
      "2 47 done, checkpoint6\n",
      "2 48 done, checkpoint6\n",
      "2 49 done, checkpoint6\n",
      "2 50 done, checkpoint6\n",
      "2 51 done, checkpoint6\n",
      "2 52 done, checkpoint6\n",
      "2 53 done, checkpoint6\n",
      "2 54 done, checkpoint6\n",
      "2 55 done, checkpoint6\n",
      "2 56 done, checkpoint6\n",
      "2 57 done, checkpoint6\n",
      "2 58 done, checkpoint6\n",
      "2 59 done, checkpoint6\n",
      "2 60 done, checkpoint6\n",
      "2 61 done, checkpoint6\n",
      "2 62 done, checkpoint6\n",
      "2 63 done, checkpoint6\n",
      "2 64 done, checkpoint6\n",
      "2 65 done, checkpoint6\n",
      "2 66 done, checkpoint6\n",
      "2 67 done, checkpoint6\n",
      "2 68 done, checkpoint6\n",
      "2 69 done, checkpoint6\n",
      "2 70 done, checkpoint6\n",
      "2 71 done, checkpoint6\n",
      "2 72 done, checkpoint6\n",
      "2 73 done, checkpoint6\n",
      "2 74 done, checkpoint6\n",
      "2 75 done, checkpoint6\n",
      "2 76 done, checkpoint6\n",
      "2 77 done, checkpoint6\n",
      "2 78 done, checkpoint6\n",
      "2 79 done, checkpoint6\n",
      "2 80 done, checkpoint6\n",
      "2 81 done, checkpoint6\n",
      "2 82 done, checkpoint6\n",
      "2 83 done, checkpoint6\n",
      "2 84 done, checkpoint6\n",
      "2 85 done, checkpoint6\n",
      "2 86 done, checkpoint6\n",
      "2 87 done, checkpoint6\n",
      "2 88 done, checkpoint6\n",
      "2 89 done, checkpoint6\n",
      "2 90 done, checkpoint6\n",
      "2 91 done, checkpoint6\n",
      "2 92 done, checkpoint6\n",
      "2 93 done, checkpoint6\n",
      "2 94 done, checkpoint6\n",
      "2 95 done, checkpoint6\n",
      "2 96 done, checkpoint6\n",
      "2 97 done, checkpoint6\n",
      "2 98 done, checkpoint6\n",
      "2 99 done, checkpoint6\n",
      "2 100 done, checkpoint6\n",
      "2 101 done, checkpoint6\n",
      "2 102 done, checkpoint6\n",
      "2 103 done, checkpoint6\n",
      "2 104 done, checkpoint6\n",
      "2 105 done, checkpoint6\n",
      "2 106 done, checkpoint6\n",
      "2 107 done, checkpoint6\n",
      "2 108 done, checkpoint6\n",
      "2 109 done, checkpoint6\n",
      "2 110 done, checkpoint6\n",
      "2 111 done, checkpoint6\n",
      "2 112 done, checkpoint6\n",
      "2 113 done, checkpoint6\n",
      "2 114 done, checkpoint6\n",
      "2 115 done, checkpoint6\n",
      "2 116 done, checkpoint6\n",
      "2 117 done, checkpoint6\n",
      "2 118 done, checkpoint6\n",
      "2 119 done, checkpoint6\n",
      "2 120 done, checkpoint6\n",
      "2 121 done, checkpoint6\n",
      "2 122 done, checkpoint6\n",
      "2 123 done, checkpoint6\n",
      "2 124 done, checkpoint6\n",
      "2 125 done, checkpoint6\n",
      "2 126 done, checkpoint6\n",
      "2 127 done, checkpoint6\n",
      "2 128 done, checkpoint6\n",
      "2 129 done, checkpoint6\n",
      "2 130 done, checkpoint6\n",
      "2 131 done, checkpoint6\n",
      "2 132 done, checkpoint6\n",
      "2 133 done, checkpoint6\n",
      "2 134 done, checkpoint6\n",
      "2 135 done, checkpoint6\n",
      "2 136 done, checkpoint6\n",
      "2 137 done, checkpoint6\n",
      "2 138 done, checkpoint6\n",
      "2 139 done, checkpoint6\n",
      "2 140 done, checkpoint6\n",
      "2 141 done, checkpoint6\n",
      "2 142 done, checkpoint6\n",
      "2 143 done, checkpoint6\n",
      "2 144 done, checkpoint6\n",
      "2 145 done, checkpoint6\n",
      "2 146 done, checkpoint6\n",
      "2 147 done, checkpoint6\n",
      "2 148 done, checkpoint6\n",
      "2 149 done, checkpoint6\n",
      "2 150 done, checkpoint6\n",
      "2 151 done, checkpoint6\n",
      "2 152 done, checkpoint6\n",
      "2 153 done, checkpoint6\n",
      "2 154 done, checkpoint6\n",
      "2 155 done, checkpoint6\n",
      "2 156 done, checkpoint6\n",
      "2 157 done, checkpoint6\n",
      "2 158 done, checkpoint6\n",
      "2 159 done, checkpoint6\n",
      "2 160 done, checkpoint6\n",
      "2 161 done, checkpoint6\n",
      "2 162 done, checkpoint6\n",
      "2 163 done, checkpoint6\n",
      "2 164 done, checkpoint6\n",
      "2 165 done, checkpoint6\n",
      "2 166 done, checkpoint6\n",
      "2 167 done, checkpoint6\n",
      "2 168 done, checkpoint6\n",
      "2 169 done, checkpoint6\n",
      "2 170 done, checkpoint6\n",
      "2 171 done, checkpoint6\n",
      "2 172 done, checkpoint6\n",
      "2 173 done, checkpoint6\n",
      "2 174 done, checkpoint6\n",
      "2 175 done, checkpoint6\n",
      "2 176 done, checkpoint6\n",
      "2 177 done, checkpoint6\n",
      "2 178 done, checkpoint6\n",
      "2 179 done, checkpoint6\n",
      "2 180 done, checkpoint6\n",
      "2 181 done, checkpoint6\n",
      "2 182 done, checkpoint6\n",
      "2 183 done, checkpoint6\n",
      "2 184 done, checkpoint6\n",
      "2 185 done, checkpoint6\n",
      "2 186 done, checkpoint6\n",
      "2 187 done, checkpoint6\n",
      "2 188 done, checkpoint6\n",
      "2 189 done, checkpoint6\n",
      "2 190 done, checkpoint6\n",
      "2 191 done, checkpoint6\n",
      "2 192 done, checkpoint6\n",
      "2 193 done, checkpoint6\n",
      "2 194 done, checkpoint6\n",
      "2 195 done, checkpoint6\n",
      "2 196 done, checkpoint6\n",
      "2 197 done, checkpoint6\n",
      "2 198 done, checkpoint6\n",
      "2 199 done, checkpoint6\n",
      "2 200 done, checkpoint6\n",
      "2 201 done, checkpoint6\n",
      "2 202 done, checkpoint6\n",
      "2 203 done, checkpoint6\n",
      "2 204 done, checkpoint6\n",
      "2 205 done, checkpoint6\n",
      "2 206 done, checkpoint6\n",
      "2 207 done, checkpoint6\n",
      "2 208 done, checkpoint6\n",
      "2 209 done, checkpoint6\n",
      "2 210 done, checkpoint6\n",
      "2 211 done, checkpoint6\n",
      "2 212 done, checkpoint6\n",
      "2 213 done, checkpoint6\n",
      "2 214 done, checkpoint6\n",
      "2 215 done, checkpoint6\n",
      "2 216 done, checkpoint6\n",
      "2 217 done, checkpoint6\n",
      "2 218 done, checkpoint6\n",
      "2 219 done, checkpoint6\n",
      "2 220 done, checkpoint6\n",
      "2 221 done, checkpoint6\n",
      "2 222 done, checkpoint6\n",
      "2 223 done, checkpoint6\n",
      "2 224 done, checkpoint6\n",
      "2 225 done, checkpoint6\n",
      "2 226 done, checkpoint6\n",
      "2 227 done, checkpoint6\n",
      "2 228 done, checkpoint6\n",
      "2 229 done, checkpoint6\n",
      "2 230 done, checkpoint6\n",
      "2 231 done, checkpoint6\n",
      "2 232 done, checkpoint6\n",
      "2 233 done, checkpoint6\n",
      "2 234 done, checkpoint6\n",
      "2 235 done, checkpoint6\n",
      "2 236 done, checkpoint6\n",
      "2 237 done, checkpoint6\n",
      "2 238 done, checkpoint6\n",
      "2 239 done, checkpoint6\n",
      "2 240 done, checkpoint6\n",
      "2 241 done, checkpoint6\n",
      "2 242 done, checkpoint6\n",
      "2 243 done, checkpoint6\n",
      "2 244 done, checkpoint6\n",
      "2 245 done, checkpoint6\n",
      "3 0 done, checkpoint6\n",
      "3 1 done, checkpoint6\n",
      "3 2 done, checkpoint6\n",
      "3 3 done, checkpoint6\n",
      "3 4 done, checkpoint6\n",
      "3 5 done, checkpoint6\n",
      "3 6 done, checkpoint6\n",
      "3 7 done, checkpoint6\n",
      "3 8 done, checkpoint6\n",
      "3 9 done, checkpoint6\n",
      "3 10 done, checkpoint6\n",
      "3 11 done, checkpoint6\n",
      "3 12 done, checkpoint6\n",
      "3 13 done, checkpoint6\n",
      "3 14 done, checkpoint6\n",
      "3 15 done, checkpoint6\n",
      "3 16 done, checkpoint6\n",
      "3 17 done, checkpoint6\n",
      "3 18 done, checkpoint6\n",
      "3 19 done, checkpoint6\n",
      "3 20 done, checkpoint6\n",
      "3 21 done, checkpoint6\n",
      "3 22 done, checkpoint6\n",
      "3 23 done, checkpoint6\n",
      "3 24 done, checkpoint6\n",
      "3 25 done, checkpoint6\n",
      "3 26 done, checkpoint6\n",
      "3 27 done, checkpoint6\n",
      "3 28 done, checkpoint6\n",
      "3 29 done, checkpoint6\n",
      "3 30 done, checkpoint6\n",
      "3 31 done, checkpoint6\n",
      "3 32 done, checkpoint6\n",
      "3 33 done, checkpoint6\n",
      "3 34 done, checkpoint6\n",
      "3 35 done, checkpoint6\n",
      "3 36 done, checkpoint6\n",
      "3 37 done, checkpoint6\n",
      "3 38 done, checkpoint6\n",
      "3 39 done, checkpoint6\n",
      "3 40 done, checkpoint6\n",
      "3 41 done, checkpoint6\n",
      "3 42 done, checkpoint6\n",
      "3 43 done, checkpoint6\n",
      "3 44 done, checkpoint6\n",
      "3 45 done, checkpoint6\n",
      "3 46 done, checkpoint6\n",
      "3 47 done, checkpoint6\n",
      "3 48 done, checkpoint6\n",
      "3 49 done, checkpoint6\n",
      "3 50 done, checkpoint6\n",
      "3 51 done, checkpoint6\n",
      "3 52 done, checkpoint6\n",
      "3 53 done, checkpoint6\n",
      "3 54 done, checkpoint6\n",
      "3 55 done, checkpoint6\n",
      "3 56 done, checkpoint6\n",
      "3 57 done, checkpoint6\n",
      "3 58 done, checkpoint6\n",
      "3 59 done, checkpoint6\n",
      "3 60 done, checkpoint6\n",
      "3 61 done, checkpoint6\n",
      "3 62 done, checkpoint6\n",
      "3 63 done, checkpoint6\n",
      "3 64 done, checkpoint6\n",
      "3 65 done, checkpoint6\n",
      "3 66 done, checkpoint6\n",
      "3 67 done, checkpoint6\n",
      "3 68 done, checkpoint6\n",
      "3 69 done, checkpoint6\n",
      "3 70 done, checkpoint6\n",
      "3 71 done, checkpoint6\n",
      "3 72 done, checkpoint6\n",
      "3 73 done, checkpoint6\n",
      "3 74 done, checkpoint6\n",
      "3 75 done, checkpoint6\n",
      "3 76 done, checkpoint6\n",
      "3 77 done, checkpoint6\n",
      "3 78 done, checkpoint6\n",
      "3 79 done, checkpoint6\n",
      "3 80 done, checkpoint6\n",
      "3 81 done, checkpoint6\n",
      "3 82 done, checkpoint6\n",
      "3 83 done, checkpoint6\n",
      "3 84 done, checkpoint6\n",
      "3 85 done, checkpoint6\n",
      "3 86 done, checkpoint6\n",
      "3 87 done, checkpoint6\n",
      "3 88 done, checkpoint6\n",
      "3 89 done, checkpoint6\n",
      "3 90 done, checkpoint6\n",
      "3 91 done, checkpoint6\n",
      "3 92 done, checkpoint6\n",
      "3 93 done, checkpoint6\n",
      "3 94 done, checkpoint6\n",
      "3 95 done, checkpoint6\n",
      "3 96 done, checkpoint6\n",
      "3 97 done, checkpoint6\n",
      "3 98 done, checkpoint6\n",
      "3 99 done, checkpoint6\n",
      "3 100 done, checkpoint6\n",
      "3 101 done, checkpoint6\n",
      "3 102 done, checkpoint6\n",
      "3 103 done, checkpoint6\n",
      "3 104 done, checkpoint6\n",
      "3 105 done, checkpoint6\n",
      "3 106 done, checkpoint6\n",
      "3 107 done, checkpoint6\n",
      "3 108 done, checkpoint6\n",
      "3 109 done, checkpoint6\n",
      "3 110 done, checkpoint6\n",
      "3 111 done, checkpoint6\n",
      "3 112 done, checkpoint6\n",
      "3 113 done, checkpoint6\n",
      "3 114 done, checkpoint6\n",
      "3 115 done, checkpoint6\n",
      "3 116 done, checkpoint6\n",
      "3 117 done, checkpoint6\n",
      "3 118 done, checkpoint6\n",
      "3 119 done, checkpoint6\n",
      "3 120 done, checkpoint6\n",
      "3 121 done, checkpoint6\n",
      "3 122 done, checkpoint6\n",
      "3 123 done, checkpoint6\n",
      "3 124 done, checkpoint6\n",
      "3 125 done, checkpoint6\n",
      "3 126 done, checkpoint6\n",
      "3 127 done, checkpoint6\n",
      "3 128 done, checkpoint6\n",
      "3 129 done, checkpoint6\n",
      "3 130 done, checkpoint6\n",
      "3 131 done, checkpoint6\n",
      "3 132 done, checkpoint6\n",
      "3 133 done, checkpoint6\n",
      "3 134 done, checkpoint6\n",
      "3 135 done, checkpoint6\n",
      "3 136 done, checkpoint6\n",
      "3 137 done, checkpoint6\n",
      "3 138 done, checkpoint6\n",
      "3 139 done, checkpoint6\n",
      "3 140 done, checkpoint6\n",
      "3 141 done, checkpoint6\n",
      "3 142 done, checkpoint6\n",
      "3 143 done, checkpoint6\n",
      "3 144 done, checkpoint6\n",
      "3 145 done, checkpoint6\n",
      "3 146 done, checkpoint6\n",
      "3 147 done, checkpoint6\n",
      "3 148 done, checkpoint6\n",
      "3 149 done, checkpoint6\n",
      "3 150 done, checkpoint6\n",
      "3 151 done, checkpoint6\n",
      "3 152 done, checkpoint6\n",
      "3 153 done, checkpoint6\n",
      "3 154 done, checkpoint6\n",
      "3 155 done, checkpoint6\n",
      "3 156 done, checkpoint6\n",
      "3 157 done, checkpoint6\n",
      "3 158 done, checkpoint6\n",
      "3 159 done, checkpoint6\n",
      "3 160 done, checkpoint6\n",
      "3 161 done, checkpoint6\n",
      "3 162 done, checkpoint6\n",
      "3 163 done, checkpoint6\n",
      "3 164 done, checkpoint6\n",
      "3 165 done, checkpoint6\n",
      "3 166 done, checkpoint6\n",
      "3 167 done, checkpoint6\n",
      "3 168 done, checkpoint6\n",
      "3 169 done, checkpoint6\n",
      "3 170 done, checkpoint6\n",
      "3 171 done, checkpoint6\n",
      "3 172 done, checkpoint6\n",
      "3 173 done, checkpoint6\n",
      "3 174 done, checkpoint6\n",
      "3 175 done, checkpoint6\n",
      "3 176 done, checkpoint6\n",
      "3 177 done, checkpoint6\n",
      "3 178 done, checkpoint6\n",
      "3 179 done, checkpoint6\n",
      "3 180 done, checkpoint6\n",
      "3 181 done, checkpoint6\n",
      "3 182 done, checkpoint6\n",
      "3 183 done, checkpoint6\n",
      "3 184 done, checkpoint6\n",
      "3 185 done, checkpoint6\n",
      "3 186 done, checkpoint6\n",
      "3 187 done, checkpoint6\n",
      "3 188 done, checkpoint6\n",
      "3 189 done, checkpoint6\n",
      "3 190 done, checkpoint6\n",
      "3 191 done, checkpoint6\n",
      "3 192 done, checkpoint6\n",
      "3 193 done, checkpoint6\n",
      "3 194 done, checkpoint6\n",
      "3 195 done, checkpoint6\n",
      "3 196 done, checkpoint6\n",
      "3 197 done, checkpoint6\n",
      "3 198 done, checkpoint6\n",
      "3 199 done, checkpoint6\n",
      "3 200 done, checkpoint6\n",
      "3 201 done, checkpoint6\n",
      "3 202 done, checkpoint6\n",
      "3 203 done, checkpoint6\n",
      "3 204 done, checkpoint6\n",
      "3 205 done, checkpoint6\n",
      "3 206 done, checkpoint6\n",
      "3 207 done, checkpoint6\n",
      "3 208 done, checkpoint6\n",
      "3 209 done, checkpoint6\n",
      "3 210 done, checkpoint6\n",
      "3 211 done, checkpoint6\n",
      "3 212 done, checkpoint6\n",
      "3 213 done, checkpoint6\n",
      "3 214 done, checkpoint6\n",
      "3 215 done, checkpoint6\n",
      "3 216 done, checkpoint6\n",
      "3 217 done, checkpoint6\n",
      "3 218 done, checkpoint6\n",
      "3 219 done, checkpoint6\n",
      "3 220 done, checkpoint6\n",
      "3 221 done, checkpoint6\n",
      "3 222 done, checkpoint6\n",
      "3 223 done, checkpoint6\n",
      "3 224 done, checkpoint6\n",
      "3 225 done, checkpoint6\n",
      "3 226 done, checkpoint6\n",
      "3 227 done, checkpoint6\n",
      "3 228 done, checkpoint6\n",
      "3 229 done, checkpoint6\n",
      "3 230 done, checkpoint6\n",
      "3 231 done, checkpoint6\n",
      "3 232 done, checkpoint6\n",
      "3 233 done, checkpoint6\n",
      "3 234 done, checkpoint6\n",
      "3 235 done, checkpoint6\n",
      "3 236 done, checkpoint6\n",
      "3 237 done, checkpoint6\n",
      "3 238 done, checkpoint6\n",
      "3 239 done, checkpoint6\n",
      "3 240 done, checkpoint6\n",
      "3 241 done, checkpoint6\n",
      "3 242 done, checkpoint6\n",
      "3 243 done, checkpoint6\n",
      "3 244 done, checkpoint6\n",
      "3 245 done, checkpoint6\n",
      "4 0 done, checkpoint6\n",
      "4 1 done, checkpoint6\n",
      "4 2 done, checkpoint6\n",
      "4 3 done, checkpoint6\n",
      "4 4 done, checkpoint6\n",
      "4 5 done, checkpoint6\n",
      "4 6 done, checkpoint6\n",
      "4 7 done, checkpoint6\n",
      "4 8 done, checkpoint6\n",
      "4 9 done, checkpoint6\n",
      "4 10 done, checkpoint6\n",
      "4 11 done, checkpoint6\n",
      "4 12 done, checkpoint6\n",
      "4 13 done, checkpoint6\n",
      "4 14 done, checkpoint6\n",
      "4 15 done, checkpoint6\n",
      "4 16 done, checkpoint6\n",
      "4 17 done, checkpoint6\n",
      "4 18 done, checkpoint6\n",
      "4 19 done, checkpoint6\n",
      "4 20 done, checkpoint6\n",
      "4 21 done, checkpoint6\n",
      "4 22 done, checkpoint6\n",
      "4 23 done, checkpoint6\n",
      "4 24 done, checkpoint6\n",
      "4 25 done, checkpoint6\n",
      "4 26 done, checkpoint6\n",
      "4 27 done, checkpoint6\n",
      "4 28 done, checkpoint6\n",
      "4 29 done, checkpoint6\n",
      "4 30 done, checkpoint6\n",
      "4 31 done, checkpoint6\n",
      "4 32 done, checkpoint6\n",
      "4 33 done, checkpoint6\n",
      "4 34 done, checkpoint6\n",
      "4 35 done, checkpoint6\n",
      "4 36 done, checkpoint6\n",
      "4 37 done, checkpoint6\n",
      "4 38 done, checkpoint6\n",
      "4 39 done, checkpoint6\n",
      "4 40 done, checkpoint6\n",
      "4 41 done, checkpoint6\n",
      "4 42 done, checkpoint6\n",
      "4 43 done, checkpoint6\n",
      "4 44 done, checkpoint6\n",
      "4 45 done, checkpoint6\n",
      "4 46 done, checkpoint6\n",
      "4 47 done, checkpoint6\n",
      "4 48 done, checkpoint6\n",
      "4 49 done, checkpoint6\n",
      "4 50 done, checkpoint6\n",
      "4 51 done, checkpoint6\n",
      "4 52 done, checkpoint6\n",
      "4 53 done, checkpoint6\n",
      "4 54 done, checkpoint6\n",
      "4 55 done, checkpoint6\n",
      "4 56 done, checkpoint6\n",
      "4 57 done, checkpoint6\n",
      "4 58 done, checkpoint6\n",
      "4 59 done, checkpoint6\n",
      "4 60 done, checkpoint6\n",
      "4 61 done, checkpoint6\n",
      "4 62 done, checkpoint6\n",
      "4 63 done, checkpoint6\n",
      "4 64 done, checkpoint6\n",
      "4 65 done, checkpoint6\n",
      "4 66 done, checkpoint6\n",
      "4 67 done, checkpoint6\n",
      "4 68 done, checkpoint6\n",
      "4 69 done, checkpoint6\n",
      "4 70 done, checkpoint6\n",
      "4 71 done, checkpoint6\n",
      "4 72 done, checkpoint6\n",
      "4 73 done, checkpoint6\n",
      "4 74 done, checkpoint6\n",
      "4 75 done, checkpoint6\n",
      "4 76 done, checkpoint6\n",
      "4 77 done, checkpoint6\n",
      "4 78 done, checkpoint6\n",
      "4 79 done, checkpoint6\n",
      "4 80 done, checkpoint6\n",
      "4 81 done, checkpoint6\n",
      "4 82 done, checkpoint6\n",
      "4 83 done, checkpoint6\n",
      "4 84 done, checkpoint6\n",
      "4 85 done, checkpoint6\n",
      "4 86 done, checkpoint6\n",
      "4 87 done, checkpoint6\n",
      "4 88 done, checkpoint6\n",
      "4 89 done, checkpoint6\n",
      "4 90 done, checkpoint6\n",
      "4 91 done, checkpoint6\n",
      "4 92 done, checkpoint6\n",
      "4 93 done, checkpoint6\n",
      "4 94 done, checkpoint6\n",
      "4 95 done, checkpoint6\n",
      "4 96 done, checkpoint6\n",
      "4 97 done, checkpoint6\n",
      "4 98 done, checkpoint6\n",
      "4 99 done, checkpoint6\n",
      "4 100 done, checkpoint6\n",
      "4 101 done, checkpoint6\n",
      "4 102 done, checkpoint6\n",
      "4 103 done, checkpoint6\n",
      "4 104 done, checkpoint6\n",
      "4 105 done, checkpoint6\n",
      "4 106 done, checkpoint6\n",
      "4 107 done, checkpoint6\n",
      "4 108 done, checkpoint6\n",
      "4 109 done, checkpoint6\n",
      "4 110 done, checkpoint6\n",
      "4 111 done, checkpoint6\n",
      "4 112 done, checkpoint6\n",
      "4 113 done, checkpoint6\n",
      "4 114 done, checkpoint6\n",
      "4 115 done, checkpoint6\n",
      "4 116 done, checkpoint6\n",
      "4 117 done, checkpoint6\n",
      "4 118 done, checkpoint6\n",
      "4 119 done, checkpoint6\n",
      "4 120 done, checkpoint6\n",
      "4 121 done, checkpoint6\n",
      "4 122 done, checkpoint6\n",
      "4 123 done, checkpoint6\n",
      "4 124 done, checkpoint6\n",
      "4 125 done, checkpoint6\n",
      "4 126 done, checkpoint6\n",
      "4 127 done, checkpoint6\n",
      "4 128 done, checkpoint6\n",
      "4 129 done, checkpoint6\n",
      "4 130 done, checkpoint6\n",
      "4 131 done, checkpoint6\n",
      "4 132 done, checkpoint6\n",
      "4 133 done, checkpoint6\n",
      "4 134 done, checkpoint6\n",
      "4 135 done, checkpoint6\n",
      "4 136 done, checkpoint6\n",
      "4 137 done, checkpoint6\n",
      "4 138 done, checkpoint6\n",
      "4 139 done, checkpoint6\n",
      "4 140 done, checkpoint6\n",
      "4 141 done, checkpoint6\n",
      "4 142 done, checkpoint6\n",
      "4 143 done, checkpoint6\n",
      "4 144 done, checkpoint6\n",
      "4 145 done, checkpoint6\n",
      "4 146 done, checkpoint6\n",
      "4 147 done, checkpoint6\n",
      "4 148 done, checkpoint6\n",
      "4 149 done, checkpoint6\n",
      "4 150 done, checkpoint6\n",
      "4 151 done, checkpoint6\n",
      "4 152 done, checkpoint6\n",
      "4 153 done, checkpoint6\n",
      "4 154 done, checkpoint6\n",
      "4 155 done, checkpoint6\n",
      "4 156 done, checkpoint6\n",
      "4 157 done, checkpoint6\n",
      "4 158 done, checkpoint6\n",
      "4 159 done, checkpoint6\n",
      "4 160 done, checkpoint6\n",
      "4 161 done, checkpoint6\n",
      "4 162 done, checkpoint6\n",
      "4 163 done, checkpoint6\n",
      "4 164 done, checkpoint6\n",
      "4 165 done, checkpoint6\n",
      "4 166 done, checkpoint6\n",
      "4 167 done, checkpoint6\n",
      "4 168 done, checkpoint6\n",
      "4 169 done, checkpoint6\n",
      "4 170 done, checkpoint6\n",
      "4 171 done, checkpoint6\n",
      "4 172 done, checkpoint6\n",
      "4 173 done, checkpoint6\n",
      "4 174 done, checkpoint6\n",
      "4 175 done, checkpoint6\n",
      "4 176 done, checkpoint6\n",
      "4 177 done, checkpoint6\n",
      "4 178 done, checkpoint6\n",
      "4 179 done, checkpoint6\n",
      "4 180 done, checkpoint6\n",
      "4 181 done, checkpoint6\n",
      "4 182 done, checkpoint6\n",
      "4 183 done, checkpoint6\n",
      "4 184 done, checkpoint6\n",
      "4 185 done, checkpoint6\n",
      "4 186 done, checkpoint6\n",
      "4 187 done, checkpoint6\n",
      "4 188 done, checkpoint6\n",
      "4 189 done, checkpoint6\n",
      "4 190 done, checkpoint6\n",
      "4 191 done, checkpoint6\n",
      "4 192 done, checkpoint6\n",
      "4 193 done, checkpoint6\n",
      "4 194 done, checkpoint6\n",
      "4 195 done, checkpoint6\n",
      "4 196 done, checkpoint6\n",
      "4 197 done, checkpoint6\n",
      "4 198 done, checkpoint6\n",
      "4 199 done, checkpoint6\n",
      "4 200 done, checkpoint6\n",
      "4 201 done, checkpoint6\n",
      "4 202 done, checkpoint6\n",
      "4 203 done, checkpoint6\n",
      "4 204 done, checkpoint6\n",
      "4 205 done, checkpoint6\n",
      "4 206 done, checkpoint6\n",
      "4 207 done, checkpoint6\n",
      "4 208 done, checkpoint6\n",
      "4 209 done, checkpoint6\n",
      "4 210 done, checkpoint6\n",
      "4 211 done, checkpoint6\n",
      "4 212 done, checkpoint6\n",
      "4 213 done, checkpoint6\n",
      "4 214 done, checkpoint6\n",
      "4 215 done, checkpoint6\n",
      "4 216 done, checkpoint6\n",
      "4 217 done, checkpoint6\n",
      "4 218 done, checkpoint6\n",
      "4 219 done, checkpoint6\n",
      "4 220 done, checkpoint6\n",
      "4 221 done, checkpoint6\n",
      "4 222 done, checkpoint6\n",
      "4 223 done, checkpoint6\n",
      "4 224 done, checkpoint6\n",
      "4 225 done, checkpoint6\n",
      "4 226 done, checkpoint6\n",
      "4 227 done, checkpoint6\n",
      "4 228 done, checkpoint6\n",
      "4 229 done, checkpoint6\n",
      "4 230 done, checkpoint6\n",
      "4 231 done, checkpoint6\n",
      "4 232 done, checkpoint6\n",
      "4 233 done, checkpoint6\n",
      "4 234 done, checkpoint6\n",
      "4 235 done, checkpoint6\n",
      "4 236 done, checkpoint6\n",
      "4 237 done, checkpoint6\n",
      "4 238 done, checkpoint6\n",
      "4 239 done, checkpoint6\n",
      "4 240 done, checkpoint6\n",
      "4 241 done, checkpoint6\n",
      "4 242 done, checkpoint6\n",
      "4 243 done, checkpoint6\n",
      "4 244 done, checkpoint6\n",
      "4 245 done, checkpoint6\n",
      "5 0 done, checkpoint6\n",
      "5 1 done, checkpoint6\n",
      "5 2 done, checkpoint6\n",
      "5 3 done, checkpoint6\n",
      "5 4 done, checkpoint6\n",
      "5 5 done, checkpoint6\n",
      "5 6 done, checkpoint6\n",
      "5 7 done, checkpoint6\n",
      "5 8 done, checkpoint6\n",
      "5 9 done, checkpoint6\n",
      "5 10 done, checkpoint6\n",
      "5 11 done, checkpoint6\n",
      "5 12 done, checkpoint6\n",
      "5 13 done, checkpoint6\n",
      "5 14 done, checkpoint6\n",
      "5 15 done, checkpoint6\n",
      "5 16 done, checkpoint6\n",
      "5 17 done, checkpoint6\n",
      "5 18 done, checkpoint6\n",
      "5 19 done, checkpoint6\n",
      "5 20 done, checkpoint6\n",
      "5 21 done, checkpoint6\n",
      "5 22 done, checkpoint6\n",
      "5 23 done, checkpoint6\n",
      "5 24 done, checkpoint6\n",
      "5 25 done, checkpoint6\n",
      "5 26 done, checkpoint6\n",
      "5 27 done, checkpoint6\n",
      "5 28 done, checkpoint6\n",
      "5 29 done, checkpoint6\n",
      "5 30 done, checkpoint6\n",
      "5 31 done, checkpoint6\n",
      "5 32 done, checkpoint6\n",
      "5 33 done, checkpoint6\n",
      "5 34 done, checkpoint6\n",
      "5 35 done, checkpoint6\n",
      "5 36 done, checkpoint6\n",
      "5 37 done, checkpoint6\n",
      "5 38 done, checkpoint6\n",
      "5 39 done, checkpoint6\n",
      "5 40 done, checkpoint6\n",
      "5 41 done, checkpoint6\n",
      "5 42 done, checkpoint6\n",
      "5 43 done, checkpoint6\n",
      "5 44 done, checkpoint6\n",
      "5 45 done, checkpoint6\n",
      "5 46 done, checkpoint6\n",
      "5 47 done, checkpoint6\n",
      "5 48 done, checkpoint6\n",
      "5 49 done, checkpoint6\n",
      "5 50 done, checkpoint6\n",
      "5 51 done, checkpoint6\n",
      "5 52 done, checkpoint6\n",
      "5 53 done, checkpoint6\n",
      "5 54 done, checkpoint6\n",
      "5 55 done, checkpoint6\n",
      "5 56 done, checkpoint6\n",
      "5 57 done, checkpoint6\n",
      "5 58 done, checkpoint6\n",
      "5 59 done, checkpoint6\n",
      "5 60 done, checkpoint6\n",
      "5 61 done, checkpoint6\n",
      "5 62 done, checkpoint6\n",
      "5 63 done, checkpoint6\n",
      "5 64 done, checkpoint6\n",
      "5 65 done, checkpoint6\n",
      "5 66 done, checkpoint6\n",
      "5 67 done, checkpoint6\n",
      "5 68 done, checkpoint6\n",
      "5 69 done, checkpoint6\n",
      "5 70 done, checkpoint6\n",
      "5 71 done, checkpoint6\n",
      "5 72 done, checkpoint6\n",
      "5 73 done, checkpoint6\n",
      "5 74 done, checkpoint6\n",
      "5 75 done, checkpoint6\n",
      "5 76 done, checkpoint6\n",
      "5 77 done, checkpoint6\n",
      "5 78 done, checkpoint6\n",
      "5 79 done, checkpoint6\n",
      "5 80 done, checkpoint6\n",
      "5 81 done, checkpoint6\n",
      "5 82 done, checkpoint6\n",
      "5 83 done, checkpoint6\n",
      "5 84 done, checkpoint6\n",
      "5 85 done, checkpoint6\n",
      "5 86 done, checkpoint6\n",
      "5 87 done, checkpoint6\n",
      "5 88 done, checkpoint6\n",
      "5 89 done, checkpoint6\n",
      "5 90 done, checkpoint6\n",
      "5 91 done, checkpoint6\n",
      "5 92 done, checkpoint6\n",
      "5 93 done, checkpoint6\n",
      "5 94 done, checkpoint6\n",
      "5 95 done, checkpoint6\n",
      "5 96 done, checkpoint6\n",
      "5 97 done, checkpoint6\n",
      "5 98 done, checkpoint6\n",
      "5 99 done, checkpoint6\n",
      "5 100 done, checkpoint6\n",
      "5 101 done, checkpoint6\n",
      "5 102 done, checkpoint6\n",
      "5 103 done, checkpoint6\n",
      "5 104 done, checkpoint6\n",
      "5 105 done, checkpoint6\n",
      "5 106 done, checkpoint6\n",
      "5 107 done, checkpoint6\n",
      "5 108 done, checkpoint6\n",
      "5 109 done, checkpoint6\n",
      "5 110 done, checkpoint6\n",
      "5 111 done, checkpoint6\n",
      "5 112 done, checkpoint6\n",
      "5 113 done, checkpoint6\n",
      "5 114 done, checkpoint6\n",
      "5 115 done, checkpoint6\n",
      "5 116 done, checkpoint6\n",
      "5 117 done, checkpoint6\n",
      "5 118 done, checkpoint6\n",
      "5 119 done, checkpoint6\n",
      "5 120 done, checkpoint6\n",
      "5 121 done, checkpoint6\n",
      "5 122 done, checkpoint6\n",
      "5 123 done, checkpoint6\n",
      "5 124 done, checkpoint6\n",
      "5 125 done, checkpoint6\n",
      "5 126 done, checkpoint6\n",
      "5 127 done, checkpoint6\n",
      "5 128 done, checkpoint6\n",
      "5 129 done, checkpoint6\n",
      "5 130 done, checkpoint6\n",
      "5 131 done, checkpoint6\n",
      "5 132 done, checkpoint6\n",
      "5 133 done, checkpoint6\n",
      "5 134 done, checkpoint6\n",
      "5 135 done, checkpoint6\n",
      "5 136 done, checkpoint6\n",
      "5 137 done, checkpoint6\n",
      "5 138 done, checkpoint6\n",
      "5 139 done, checkpoint6\n",
      "5 140 done, checkpoint6\n",
      "5 141 done, checkpoint6\n",
      "5 142 done, checkpoint6\n",
      "5 143 done, checkpoint6\n",
      "5 144 done, checkpoint6\n",
      "5 145 done, checkpoint6\n",
      "5 146 done, checkpoint6\n",
      "5 147 done, checkpoint6\n",
      "5 148 done, checkpoint6\n",
      "5 149 done, checkpoint6\n",
      "5 150 done, checkpoint6\n",
      "5 151 done, checkpoint6\n",
      "5 152 done, checkpoint6\n",
      "5 153 done, checkpoint6\n",
      "5 154 done, checkpoint6\n",
      "5 155 done, checkpoint6\n",
      "5 156 done, checkpoint6\n",
      "5 157 done, checkpoint6\n",
      "5 158 done, checkpoint6\n",
      "5 159 done, checkpoint6\n",
      "5 160 done, checkpoint6\n",
      "5 161 done, checkpoint6\n",
      "5 162 done, checkpoint6\n",
      "5 163 done, checkpoint6\n",
      "5 164 done, checkpoint6\n",
      "5 165 done, checkpoint6\n",
      "5 166 done, checkpoint6\n",
      "5 167 done, checkpoint6\n",
      "5 168 done, checkpoint6\n",
      "5 169 done, checkpoint6\n",
      "5 170 done, checkpoint6\n",
      "5 171 done, checkpoint6\n",
      "5 172 done, checkpoint6\n",
      "5 173 done, checkpoint6\n",
      "5 174 done, checkpoint6\n",
      "5 175 done, checkpoint6\n",
      "5 176 done, checkpoint6\n",
      "5 177 done, checkpoint6\n",
      "5 178 done, checkpoint6\n",
      "5 179 done, checkpoint6\n",
      "5 180 done, checkpoint6\n",
      "5 181 done, checkpoint6\n",
      "5 182 done, checkpoint6\n",
      "5 183 done, checkpoint6\n",
      "5 184 done, checkpoint6\n",
      "5 185 done, checkpoint6\n",
      "5 186 done, checkpoint6\n",
      "5 187 done, checkpoint6\n",
      "5 188 done, checkpoint6\n",
      "5 189 done, checkpoint6\n",
      "5 190 done, checkpoint6\n",
      "5 191 done, checkpoint6\n",
      "5 192 done, checkpoint6\n",
      "5 193 done, checkpoint6\n",
      "5 194 done, checkpoint6\n",
      "5 195 done, checkpoint6\n",
      "5 196 done, checkpoint6\n",
      "5 197 done, checkpoint6\n",
      "5 198 done, checkpoint6\n",
      "5 199 done, checkpoint6\n",
      "5 200 done, checkpoint6\n",
      "5 201 done, checkpoint6\n",
      "5 202 done, checkpoint6\n",
      "5 203 done, checkpoint6\n",
      "5 204 done, checkpoint6\n",
      "5 205 done, checkpoint6\n",
      "5 206 done, checkpoint6\n",
      "5 207 done, checkpoint6\n",
      "5 208 done, checkpoint6\n",
      "5 209 done, checkpoint6\n",
      "5 210 done, checkpoint6\n",
      "5 211 done, checkpoint6\n",
      "5 212 done, checkpoint6\n",
      "5 213 done, checkpoint6\n",
      "5 214 done, checkpoint6\n",
      "5 215 done, checkpoint6\n",
      "5 216 done, checkpoint6\n",
      "5 217 done, checkpoint6\n",
      "5 218 done, checkpoint6\n",
      "5 219 done, checkpoint6\n",
      "5 220 done, checkpoint6\n",
      "5 221 done, checkpoint6\n",
      "5 222 done, checkpoint6\n",
      "5 223 done, checkpoint6\n",
      "5 224 done, checkpoint6\n",
      "5 225 done, checkpoint6\n",
      "5 226 done, checkpoint6\n",
      "5 227 done, checkpoint6\n",
      "5 228 done, checkpoint6\n",
      "5 229 done, checkpoint6\n",
      "5 230 done, checkpoint6\n",
      "5 231 done, checkpoint6\n",
      "5 232 done, checkpoint6\n",
      "5 233 done, checkpoint6\n",
      "5 234 done, checkpoint6\n",
      "5 235 done, checkpoint6\n",
      "5 236 done, checkpoint6\n",
      "5 237 done, checkpoint6\n",
      "5 238 done, checkpoint6\n",
      "5 239 done, checkpoint6\n",
      "5 240 done, checkpoint6\n",
      "5 241 done, checkpoint6\n",
      "5 242 done, checkpoint6\n",
      "5 243 done, checkpoint6\n",
      "5 244 done, checkpoint6\n",
      "5 245 done, checkpoint6\n"
     ]
    }
   ],
   "source": [
    "# 전체 그룹 roi 로드 후 neuralRSM 만들기\n",
    "results = np.zeros((4,3,246,2)) # group,sub,roi,shared/random\n",
    "neuralRSMs = np.zeros_like(group_brain, dtype='object')\n",
    "subj_combinations = list(itertools.combinations(range(3), 2))\n",
    "\n",
    "for group in [2,3,4,5]:\n",
    "    lastidx = []\n",
    "    recalled_evidx_all_oh, recalled_evs_all = extract_recalledidx_all_sharedeb(int(group))\n",
    "    recalled_evs = extract_recalledidx(int(group))\n",
    "    \n",
    "    for roi in range(246): #246\n",
    "        event_patterns = np.zeros((3, len(recalled_evidx_all_oh), group_brain[int(group)-2,int(sub)-1][run-1][roi].shape[1])) #\n",
    "         \n",
    "        for sub in [1,2,3]:\n",
    "            posthoc_data = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV_clean/group-0{group}/sub-0{group}0{sub}_day2_posthoc.xlsx')\n",
    "            segcol = np.array(posthoc_data['segmentation'])\n",
    "            lastidx.append(max(segcol))\n",
    "            \n",
    "        for sub in [1,2,3]:\n",
    "            if (group==2 and sub==1):\n",
    "                runs = [1,2,3]\n",
    "            else:\n",
    "                runs = [1]\n",
    "            eventstartend = []\n",
    "            runidx = []\n",
    "            for run in runs:\n",
    "                evse = get_boundary2(group,sub,run)\n",
    "                eventstartend.extend(evse)\n",
    "                runidx.extend([run]*len(evse))\n",
    "\n",
    "            # 모든 사건 패턴 로드 후 event_patterns[sub-1]에 넣어주기\n",
    "            for e,ev in enumerate(recalled_evs_all):\n",
    "                if recalled_evidx_all_oh[e][sub-1] == 1:\n",
    "                    event_no = str(ev[sub]).split(',') #list of str\n",
    "                    if len(event_no) > 1:\n",
    "                        single_pattern_timepoint = eventstartend[int(float(event_no[0]))-1]\n",
    "                        for efr in event_no[1:]:\n",
    "                            single_pattern_timepoint = np.vstack((single_pattern_timepoint,eventstartend[int(float(efr))-1]))\n",
    "                        single_pattern = group_brain[group-2,sub-1][runidx[int(float(event_no[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0,0]/1000):math.ceil(single_pattern_timepoint[0,1]/1000),:]\n",
    "                        for s in range(1, single_pattern_timepoint.shape[-1]):\n",
    "                            single_pattern = np.vstack((single_pattern,group_brain[group-2,sub-1][runidx[math.trunc(float(event_no[0]))-1]-1][roi][math.ceil(single_pattern_timepoint[s,0]/1000):int(single_pattern_timepoint[s,1]/1000),:]))\n",
    "                    else:\n",
    "                        single_pattern_timepoint = eventstartend[int(float(event_no[0]))-1]\n",
    "                        single_pattern = group_brain[group-2,sub-1][runidx[int(float(event_no[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0]/1000):math.ceil(single_pattern_timepoint[1]/1000),:]\n",
    "                    event_pattern = np.mean(single_pattern,axis=0)\n",
    "                    event_patterns[sub-1,recalled_evs_all[e,0]-1,:] = event_pattern\n",
    "                        \n",
    "            # 이제 neuralRSM 만들기 +냅다 다 저장\n",
    "            neuralRSM = np.zeros((max(recalled_evs_all[:,0]),max(recalled_evs_all[:,0])))\n",
    "            for i in range(len(neuralRSM)):\n",
    "                for j in range(i, len(neuralRSM)):\n",
    "                    if recalled_evidx_all_oh[i,sub-1]==1 and recalled_evidx_all_oh[j,sub-1]==1:\n",
    "                        neuralRSM[i, j] = pearsonr(event_patterns[sub-1,i,:],event_patterns[sub-1,j,:])[0]\n",
    "            #print(neuralRSM.shape, 'checkpoint4') #이게 각 사람의 neuralRDM\n",
    "            neuralRSMs[group-2,sub-1] = neuralRSM\n",
    "\n",
    "        # 한 그룹 다 저장 후 neuralRSM spearman 계산\n",
    "        for sc in subj_combinations:\n",
    "            nRf1 = np.triu(neuralRSMs[group-2,sc[0]]).flatten()\n",
    "            nRf2 = np.triu(neuralRSMs[group-2,sc[1]]).flatten()\n",
    "            nRf3 = nRf2[np.random.permutation(nRf2.size)]\n",
    "            results[group-2,sc[0],roi,0] = spearmanr(nRf1,nRf2)[0]\n",
    "            results[group-2,sc[0],roi,1] = spearmanr(nRf1,nRf3)[0]\n",
    "        print(group, roi,'done, checkpoint6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n",
      "(12,) (12,)\n"
     ]
    }
   ],
   "source": [
    "result_random = np.zeros((246,2))\n",
    "for roi in range(246):\n",
    "    sharedr = np.concatenate([results[0,:,roi,0],results[1,:,roi,0],results[2,:,roi,0],results[3,:,roi,0]])\n",
    "    randomr = np.concatenate([results[0,:,roi,1],results[1,:,roi,1],results[2,:,roi,1],results[3,:,roi,1]])\n",
    "    result_random[roi,:] = ttest_ind(r2z(sharedr),r2z(randomr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   8   9  10  12  13  14  15  16  17  18  19\n",
      "  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  60  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 105 106 107 108 109 110 111 112 114 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133 135 136 137\n",
      " 139 140 141 142 143 145 146 147 148 149 150 151 152 153 154 155 156 158\n",
      " 159 160 161 162 163 164 165 166 168 169 170 171 172 173 174 175 177 178\n",
      " 179 181 182 183 184 185 186 187 188 189 190 191 192 193 194 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 209 210 211 212 214 215 216 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245]\n",
      "[  0   1   2   3   4   5   6   8   9  10  12  13  14  15  16  17  18  19\n",
      "  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  60  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 105 106 107 108 109 110 111 112 114 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133 135 136 137\n",
      " 139 140 141 142 143 145 146 148 149 150 151 152 153 154 155 156 158 159\n",
      " 160 161 162 163 164 165 166 168 169 170 171 172 173 174 175 177 178 179\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 196 197 198 199\n",
      " 200 201 202 203 204 205 206 207 209 210 211 212 214 215 216 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(result_random[:,1]<0.05)[0])\n",
    "corrected_p_random = np.zeros_like(result_random[:,1])\n",
    "corrected_p_random = multitest.multipletests(result_random[:,1], method='fdr_bh')[1]\n",
    "print(np.where(corrected_p_random<0.05)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = np.array(nib.load(f'{MASKDIR}BNA_3mm_atlas.nii').get_fdata())\n",
    "\n",
    "empty_brain = np.zeros((atlas.shape))\n",
    "for i in range(1,247):\n",
    "    if corrected_p_random[i-1] < 0.05:\n",
    "        empty_brain[atlas==i] = result_random[i-1,0]\n",
    "empty_brain[empty_brain==0.0] = np.nan\n",
    "np.save(\"output/230611 isrsa_tmap.npy\", empty_brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1495894893029552"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(result_random[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
