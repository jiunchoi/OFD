{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "\n",
    "from scipy.stats import zscore, pearsonr, kendalltau,  entropy, spearmanr, linregress, rankdata, ttest_rel, ttest_1samp, ttest_ind\n",
    "from statsmodels.stats import multitest\n",
    "import scipy.linalg as la\n",
    "import itertools\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/home/jiunchoi/OFD/OFD_BHV_clean'\n",
    "group = 2\n",
    "sub = 1\n",
    "groupsub = '0'+str(group)+'0'+str(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_recalledidx(group):\n",
    "    keys = [f'sub-0{group}01',f'sub-0{group}02',f'sub-0{group}03']\n",
    "\n",
    "    data_transcript = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV_clean/group-0{str(group)}/group-0{str(group)} sharedeb.xlsx')\n",
    "    sharedeb = np.array(data_transcript['sharedeb'])\n",
    "    data_recs = np.array(data_transcript[keys])\n",
    "\n",
    "    recalled_evidx=[]\n",
    "    recalled_evs = []\n",
    "    for i in range(len(sharedeb)):\n",
    "        count=0\n",
    "        for j in range(3):\n",
    "            if (type(data_recs[i,j])==str or np.isnan(data_recs[i,j])==False) and (data_recs[i,j]!=0):\n",
    "                count+=1\n",
    "        if count==3:\n",
    "            if sharedeb[i] not in recalled_evidx:\n",
    "                recalled_evidx.append(sharedeb[i])\n",
    "                recalled_evs.append(data_recs[i,:])\n",
    "    return recalled_evs\n",
    "\n",
    "def get_boundary(group,sub,run):\n",
    "    groupsub = '0'+str(group)+'0'+str(sub)\n",
    "    word_data = pd.read_excel(f'/home/jiunchoi/OFD/OFD_AUDIO/derivatives/group-0{group}/sub-{groupsub}_run-{run}_day2_words.xlsx')\n",
    "    posthoc_data = pd.read_excel(f'/home/jiunchoi/OFD/OFD_BHV_clean/group-0{group}/sub-{groupsub}_day2_posthoc.xlsx')\n",
    "    # word_data.keys() = start, end, word\n",
    "    # posthoc_data.keys() = sentence, segmentation, description, tag\n",
    "    words = word_data['word']\n",
    "    start = word_data['start']\n",
    "    end = word_data['end']\n",
    "    sentstartend = []\n",
    "    eventstartend = []\n",
    "    endidx = [0]\n",
    "    for t in range (len(posthoc_data['Unnamed: 0'])-1):\n",
    "        if posthoc_data['Unnamed: 0'][t]+1 != posthoc_data['Unnamed: 0'][t+1]:\n",
    "            endidx.append(t)\n",
    "    endidx.append(len(posthoc_data['Unnamed: 0'])-1)\n",
    "    if len(endidx)>2:\n",
    "        segcol = np.array(posthoc_data['segmentation'][endidx[run-1]+1:endidx[run]+1])\n",
    "    else:\n",
    "        segcol = np.array(posthoc_data['segmentation'])\n",
    "\n",
    "    tmpstart = start[0]\n",
    "    for i in range (len(words)-1):\n",
    "        if '.' in str(words[i]) or ',' in str(words[i]) or '었고' in str(words[i]) or '것 같고' in str(words[i]) or '지고' in str(words[i]) or '는데' in str(words[i]) or '근데' in str(words[i]) or '그리고' in str(words[i]) or '그러고' in str(words[i]) or '됐고' in str(words[i]) or '했고' in str(words[i]): \n",
    "            tmpend = end[i]\n",
    "            sentstartend.append([tmpstart,tmpend])\n",
    "            tmpstart = start[i+1]\n",
    "    tmpend = end[len(words)-1]\n",
    "    sentstartend.append([tmpstart,tmpend])\n",
    "\n",
    "    # working for subs with only 1 run\n",
    "    tmpstart = sentstartend[0][0]\n",
    "    for i in range (len(segcol)-1):\n",
    "        if segcol[i] != segcol[i+1]:     \n",
    "            tmpend = sentstartend[i][1]     \n",
    "            eventstartend.append([tmpstart,tmpend])\n",
    "            tmpstart = sentstartend[i+1][0]\n",
    "    tmpend = sentstartend[len(segcol)-1][1]\n",
    "    eventstartend.append([tmpstart,tmpend]) \n",
    "    \n",
    "    return eventstartend\n",
    "\n",
    "def get_motion_params(subn,task,run):\n",
    "    fname = f'/home/jiunchoi/OFD/OFD_DATA/derivatives/sub-{subn}/func/sub-{subn}_task-{task}_run-{run}_desc-confounds_timeseries.tsv'\n",
    "    compounds = pd.read_csv(fname, delimiter='\\t')\n",
    "\n",
    "    confound_labels =  ['trans_x', \n",
    "                    'trans_y', \n",
    "                    'trans_z',\n",
    "                    'rot_x',\n",
    "                    'rot_y',\n",
    "                    'rot_z',\n",
    "                    'trans_x_derivative1',\n",
    "                    'trans_y_derivative1',\n",
    "                    'trans_z_derivative1',\n",
    "                    'rot_x_derivative1',\n",
    "                    'rot_y_derivative1',\n",
    "                    'rot_z_derivative1',\n",
    "                    'trans_x_power2',\n",
    "                    'trans_y_power2',\n",
    "                    'trans_z_power2',\n",
    "                    'rot_x_power2',\n",
    "                    'rot_y_power2',\n",
    "                    'rot_z_power2',\n",
    "                    'trans_x_derivative1_power2',\n",
    "                    'trans_y_derivative1_power2',\n",
    "                    'trans_z_derivative1_power2',\n",
    "                    'rot_x_derivative1_power2',\n",
    "                    'rot_y_derivative1_power2',\n",
    "                    'rot_z_derivative1_power2']\n",
    "    X_motion = compounds[confound_labels]\n",
    "    X_motion = np.nan_to_num(X_motion)\n",
    "    return X_motion\n",
    "\n",
    "def mot_regout(groupsub,task,run,data):\n",
    "    motparam = get_motion_params(groupsub, task, run)\n",
    "\n",
    "    data_mean = data.mean(0)\n",
    "    data = data - data_mean\n",
    "    coef, _, _, _ = la.lstsq(motparam, data)\n",
    "    # remove trends and add back mean of the data\n",
    "    data_clean = data - motparam.dot(coef) + data_mean\n",
    "\n",
    "    return data_clean\n",
    "\n",
    "\n",
    "MASKDIR = '/home/jiunchoi/OFD/source/'\n",
    "DATADIR = '/home/jiunchoi/OFD/OFD_DATA/derivatives/'\n",
    "BN_atlas = nib.load(f'{MASKDIR}BNA_3mm_atlas.nii').get_fdata()\n",
    "\n",
    "def load_brain(group,sub,run):\n",
    "    task = 'RECALL'\n",
    "    BN_parcels = np.zeros((246), dtype='object')\n",
    "    groupsub = '0'+str(group)+'0'+str(sub)\n",
    "    fmri_data = nib.load(f'{DATADIR}sub-{groupsub}/preprocessed/sub-{groupsub}_{task}_{run}_sc_dt_sm.nii.gz').get_fdata()\n",
    "    for roi in range (1,246+1):\n",
    "        roi_data = fmri_data[BN_atlas==roi, :].T #(Time, Voxels)\n",
    "        roi_data = zscore(roi_data, axis=0)\n",
    "        roi_data = np.nan_to_num(roi_data)\n",
    "        BN_parcels[roi-1] = roi_data\n",
    "    return BN_parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=[2,3,4,5]\n",
    "group_brain = np.zeros((len(groups),3), dtype='object')\n",
    "\n",
    "for group in groups:\n",
    "    for sub in [1,2,3]:\n",
    "        sub_brain = []\n",
    "        if (group==2 and sub==1):\n",
    "            runs = [1,2,3]\n",
    "        else:\n",
    "            runs = [1]\n",
    "        for run in runs:\n",
    "            sub_brain.append(load_brain(group,sub,run))\n",
    "        group_brain[group-2,sub-1] = sub_brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiunchoi/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jiunchoi/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/home/jiunchoi/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jiunchoi/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "groups=[2,3,4,5]; roi=245\n",
    "\n",
    "group_evpatterns = []\n",
    "for group in groups:\n",
    "    recalled_evs = extract_recalledidx(group)\n",
    "    event_patterns = np.zeros_like(recalled_evs,dtype='object')\n",
    "    for sub in [1,2,3]:\n",
    "        if (group==2 and sub==1):\n",
    "            runs = [1,2,3]\n",
    "        else:\n",
    "            runs = [1]\n",
    "        eventstartend = []\n",
    "        for run in runs:\n",
    "            eventstartend.extend(get_boundary(group,sub,run))\n",
    "\n",
    "        for e in range(len(recalled_evs)):\n",
    "            ev_from_recall = str(recalled_evs[e][sub-1]).split(',')\n",
    "            if len(ev_from_recall) > 1:\n",
    "                single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                for efr in ev_from_recall[1:]:\n",
    "                    single_pattern_timepoint = np.vstack((single_pattern_timepoint,eventstartend[int(efr)-1]))\n",
    "                single_pattern = group_brain[group-2,sub-1][run-1][roi][int(single_pattern_timepoint[0,0]/1000):int(single_pattern_timepoint[0,1]/1000),:]\n",
    "                for s in range(1, single_pattern_timepoint.shape[-1]):\n",
    "                    single_pattern = np.vstack((single_pattern,group_brain[group-2,sub-1][run-1][roi][int(single_pattern_timepoint[s,0]/1000):int(single_pattern_timepoint[s,1]/1000),:]))\n",
    "            else:\n",
    "                single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                single_pattern = group_brain[group-2,sub-1][run-1][roi][int(single_pattern_timepoint[0]/1000):int(single_pattern_timepoint[1]/1000),:]\n",
    "            event_pattern = np.mean(single_pattern,axis=0)\n",
    "            event_patterns[e,sub-1] = event_pattern\n",
    "    group_evpatterns.append(event_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [0,1,2]\n",
    "eventwise_rs = np.zeros((246,len(groups)),dtype='object')\n",
    "\n",
    "menu_evs = [[8],[10,11,12,14,15,16,17],[3,4],[5,6,7,8,9,10,11,12]] #순서대로 group\n",
    "task2_evs = [[9,10,11,12,13,14,15,16],[8,9,18,19],['nan'],[4,14,15,16]]\n",
    "task13_evs = []\n",
    "\n",
    "for g,group in enumerate(groups):\n",
    "    recalled_evs = extract_recalledidx(group)\n",
    "    task13_ev = [i for i in range(len(recalled_evs))]\n",
    "    subtracted = [x for x in task13_ev if x not in menu_evs[g] and task2_evs[g]]\n",
    "    task13_evs.append(subtracted)\n",
    "\n",
    "entire_roigroups_evpatterns = np.zeros((246,len(groups)),dtype='object') #roi, group, sub, event\n",
    "\n",
    "for roi in range(246):\n",
    "    for group in groups:\n",
    "        recalled_evs = extract_recalledidx(group)\n",
    "        event_patterns = np.zeros_like(recalled_evs,dtype='object')\n",
    "        for sub in [1,2,3]:\n",
    "            if (group==2 and sub==1):\n",
    "                runs = [1,2,3]\n",
    "            else:\n",
    "                runs = [1]\n",
    "            eventstartend = []\n",
    "            runidx = []\n",
    "            for run in runs:\n",
    "                evse = get_boundary(group,sub,run)\n",
    "                eventstartend.extend(evse)\n",
    "                runidx.extend([run]*len(evse))\n",
    "                \n",
    "            for e in range(len(recalled_evs)):\n",
    "                ev_from_recall = str(recalled_evs[e][sub-1]).split(',')\n",
    "\n",
    "                if len(ev_from_recall) > 1:\n",
    "                    single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                    for efr in ev_from_recall[1:]:\n",
    "                        single_pattern_timepoint = np.vstack((single_pattern_timepoint,eventstartend[int(float(efr))-1]))\n",
    "                    single_pattern = group_brain[group-2,sub-1][runidx[int(float(ev_from_recall[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0,0]/1000):math.ceil(single_pattern_timepoint[0,1]/1000),:]\n",
    "                    for s in range(1, single_pattern_timepoint.shape[-1]):\n",
    "                        single_pattern = np.vstack((single_pattern,group_brain[group-2,sub-1][runidx[math.trunc(float(ev_from_recall[0]))-1]-1][roi][math.ceil(single_pattern_timepoint[s,0]/1000):int(single_pattern_timepoint[s,1]/1000),:]))\n",
    "                else:\n",
    "                    single_pattern_timepoint = eventstartend[int(float(ev_from_recall[0]))-1]\n",
    "                    single_pattern = group_brain[group-2,sub-1][runidx[int(float(ev_from_recall[0]))-1]-1][roi][math.trunc(single_pattern_timepoint[0]/1000):math.ceil(single_pattern_timepoint[1]/1000),:]\n",
    "                event_pattern = np.mean(single_pattern,axis=0)\n",
    "                event_patterns[e,sub-1] = event_pattern\n",
    "            entire_roigroups_evpatterns[roi,group-2] = event_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ISPC\n",
    "eventwise_rs = np.zeros((246,len(groups)),dtype='object')\n",
    "for roi in range(246):\n",
    "    for group in groups:\n",
    "        eventwise_r = np.zeros((len(entire_roigroups_evpatterns[roi][group-2]),3))\n",
    "        for e in range(len(eventwise_r)):\n",
    "            for j in range (3):\n",
    "                a = idx[j-2]\n",
    "                b = idx[j-1]\n",
    "                c = idx[j]\n",
    "            \n",
    "                pattern_1 = entire_roigroups_evpatterns[roi][group-2][e,a]\n",
    "                pattern_2 = np.mean((entire_roigroups_evpatterns[roi][group-2][e,b],entire_roigroups_evpatterns[roi][group-2][e,c]),axis=0)\n",
    "                eventwise_r[e,a] = pearsonr(pattern_1, pattern_2)[0]\n",
    "        eventwise_rs[roi,group-2] = eventwise_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutation = 1000\n",
    "permuted_stats = np.zeros((n_permutation,2))\n",
    "\n",
    "for i in range(n_permutation):\n",
    "    chosen_evs = np.zeros((4,3)) #group, conds\n",
    "    chosen_evpatterns = np.zeros((4,3,2)) #group, rs, conditions\n",
    "    for group in groups:\n",
    "        chosen_evs[group-2,0] = random.choice(menu_evs[group-2]) #menu\n",
    "        # chosen_evs[group-2,1] = random.choice(task2_evs[group-2]) #task2\n",
    "        chosen_evs[group-2,2] = random.choice(task13_evs[group-2]) #task13\n",
    "        chosen_evpatterns[group-2,:,0] = eventwise_rs[roi,group-2][int(chosen_evs[group-2,0])]\n",
    "        chosen_evpatterns[group-2,:,1] = eventwise_rs[roi,group-2][int(chosen_evs[group-2,2])]\n",
    "\n",
    "    group1 = chosen_evpatterns[:,:,0].flatten()\n",
    "    group2 = chosen_evpatterns[:,:,1].flatten()\n",
    "\n",
    "    t_stat, p_values = stats.ttest_ind(group1, group2)\n",
    "    corrected_p_values = multitest.multipletests(p_values, method='fdr_bh')[1]\n",
    "    permuted_stats[i,:] = [tstat, corrected_p_values]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cortex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
